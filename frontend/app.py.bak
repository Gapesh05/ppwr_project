"""Frontend Flask application for PFAS/PPWR UI."""

from flask import Flask, render_template, request, redirect, url_for, flash, jsonify, send_file, session
import os
from config import Config
from io import BytesIO
import pandas as pd
import logging
from logging.handlers import RotatingFileHandler
from openpyxl.styles import PatternFill
import requests
from flask_sqlalchemy import SQLAlchemy
from models import db as models_db, PFASBOM, PFASMaterialChemicals, PFASRegulations, PFASBOMAudit, SupplierDeclaration, MaterialDeclarationLink, PPWRBOM, PPWRMaterialDeclarationLink
from fastapi_client import (
    ingest_material_data,
    upload_supplier_declaration as fastapi_upload_supplier_declaration,
    list_supplier_declarations as fastapi_list_supplier_declarations,
    assess_from_declaration as fastapi_assess_from_declaration,
    get_assessments as fastapi_get_assessments,
    map_supplier_declaration as fastapi_map_supplier_declaration,
    get_ppwr_evaluation_summary as fastapi_get_ppwr_evaluation_summary,
)
import json
from datetime import datetime
import sqlalchemy
from sqlalchemy import text
from flask import Response   # if you want template download route returning CSV
from decimal import Decimal, InvalidOperation
from werkzeug.utils import secure_filename
import uuid
import re
import tempfile
import shutil
import time

# ==================== FLASK APP CONFIG ====================

app = Flask(__name__)
app.config.from_object(Config)
app.secret_key = 'your-super-secret-key'

# Enable debug mode
app.config['DEBUG'] = True
app.config['SECRET_KEY'] = os.environ.get('SECRET_KEY', 'dev-secret-key')

# Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
app.logger.setLevel(logging.INFO)

# Write logs to a rotating file as well (frontend/logs/app.log)
try:
    logs_dir = os.path.join(os.path.dirname(__file__), 'logs')
    os.makedirs(logs_dir, exist_ok=True)
    file_handler = RotatingFileHandler(
        os.path.join(logs_dir, 'app.log'),
        maxBytes=2 * 1024 * 1024,  # 2 MB per file
        backupCount=5,
        encoding='utf-8'
    )
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(message)s'))
    app.logger.addHandler(file_handler)
    # Also attach to root logger so SQLAlchemy and werkzeug messages go to file
    logging.getLogger().addHandler(file_handler)
except Exception as _e:
    # Fallback: continue without file handler
    pass

# Database
# Use the SQLAlchemy instance defined in `models.py` so models' metadata is registered
db = models_db
db.init_app(app)
# Ensure SQLAlchemy sessions are removed at the end of each request/context
@app.teardown_appcontext
def shutdown_session(exception=None):
    try:
        db.session.remove()
    except Exception:
        pass

# PPWR: standalone evaluation page route
@app.route('/ppwr/evaluation')
def ppwr_assessment_evaluation_page():
    """
    Render the standalone PPWR assessment evaluation UI page.
    This route is for the dedicated evaluation UI, not tied to a specific SKU or declaration.
    """
    try:
        payload = fastapi_get_ppwr_evaluation_summary()
        stats = (payload or {}).get('stats') or {}
        rows = (payload or {}).get('rows') or []
        return render_template('ppwr_assessment_evaluation.html', stats=stats, rows=rows)
    except Exception as e:
        app.logger.error(f"Error rendering PPWR assessment evaluation page: {e}", exc_info=True)
        return "Error loading evaluation page", 500
# Defer table creation to guarded startup block in __main__ to avoid
# failing import when DB is temporarily unreachable.

# Directory where supplier declaration files are stored (on the frontend service filesystem)
SUPPLIER_DECLARATION_DIR = os.path.join(app.root_path, 'supplier_declarations')
os.makedirs(SUPPLIER_DECLARATION_DIR, exist_ok=True)

# Allowed file extensions for declarations
ALLOWED_DECL_EXT = {'.pdf', '.txt', '.csv', '.xls', '.xlsx', '.doc', '.docx'}

def _allowed_decl(filename: str) -> bool:
    if not filename:
        return False
    _, ext = os.path.splitext(filename.lower())
    return ext in ALLOWED_DECL_EXT


# Quiet favicon 404s to keep console clean during testing
@app.route('/favicon.ico')
def _favicon():
    try:
        static_favicon = os.path.join(app.root_path, 'static', 'favicon.ico')
        if os.path.exists(static_favicon):
            return send_file(static_favicon)
    except Exception:
        pass
    return ('', 204)


def _normalize_material_value(raw_val: object) -> str:
    """Normalize a material cell value to a stable material_id-like token.

    Handles multi-line cells like "1073B Tyvek\nA8362" by selecting the most ID-like token (usually the last short token).
    Falls back to the stripped raw value.
    """
    if raw_val is None:
        return ''
    try:
        s = str(raw_val).strip()
    except Exception:
        return ''

    if s == '' or s.lower() == 'nan':
        return ''

    # Split on newlines first, then common separators
    parts = [p.strip() for p in re.split(r'[\r\n]+', s) if p.strip()]
    if len(parts) == 0:
        return ''
    if len(parts) == 1:
        candidate = parts[0]
    else:
        # Prefer the last part if it looks ID-like (short, alphanumeric)
        last = parts[-1]
        if re.match(r'^[A-Za-z0-9_\-]{1,40}$', last):
            candidate = last
        else:
            # otherwise prefer the shortest part (likely an ID)
            candidate = min(parts, key=lambda x: len(x))

    # Further sanitize: remove surrounding parentheses and stray characters
    candidate = candidate.strip()
    candidate = re.sub(r'^[\(\[\{\s]+|[\)\]\}\s]+$', '', candidate)

    # If candidate contains a space and also contains a short token, pick that
    if ' ' in candidate:
        tokens = [t.strip() for t in re.split(r'[\s/,_]+', candidate) if t.strip()]
        for t in tokens[::-1]:
            if re.match(r'^[A-Za-z0-9_\-]{1,40}$', t):
                candidate = t
                break

    return candidate



# ==================== ROUTES ====================


@app.route('/')
def index():
    app.logger.info("Loading index page")
    try:
        # Query distinct products (one row per SKU)
        results = db.session.query(
            PFASBOM.sku,
            PFASBOM.product,
            PFASBOM.portal_name,
            PFASBOM.region,
            PFASBOM.assessment
        ).distinct(PFASBOM.sku).all()
        
        app.logger.info(f"Found {len(results)} distinct products")

        # Get the latest persisted upload time per SKU from the BOM table
        from sqlalchemy import func
        sku_to_uploaded = dict(
            db.session.query(PFASBOM.sku, func.max(PFASBOM.uploaded_at)).group_by(PFASBOM.sku).all()
        )

        # Check if there was a recent upload to highlight (used as a fallback only)
        last_upload = session.pop('last_upload', None)  # contains {'timestamp': str, 'skus': [..]} or None
        uploaded_ts = None
        uploaded_skus = set()
        if last_upload:
            uploaded_ts = last_upload.get('timestamp')
            uploaded_skus = set(last_upload.get('skus', []))
            app.logger.info(f"Applying last_upload timestamp {uploaded_ts} to SKUs: {uploaded_skus}")

        products = []
        for row in results:
            sku_val = row.sku
            # Prefer the persisted max(uploaded_at) from DB; if not available, use the recent session timestamp
            persisted_dt = sku_to_uploaded.get(sku_val)
            if persisted_dt:
                try:
                    uploaded_at_str = persisted_dt.strftime("%Y-%m-%d %H:%M:%S UTC")
                except Exception:
                    uploaded_at_str = str(persisted_dt)
            else:
                uploaded_at_str = uploaded_ts if sku_val in uploaded_skus else None

            products.append({
                'sku': sku_val,
                'product_name': f"{sku_val}_{row.product}",
                'portal_name': row.portal_name,
                'region': row.region or "Global",
                'assessment': row.assessment or "PFAS",
                'uploaded_at': uploaded_at_str
            })
        
        return render_template('index.html', products=products)
    except Exception as e:
        app.logger.error(f"Error loading products: {e}", exc_info=True)
        flash("Error loading products", "danger")
        return render_template('index.html', products=[])

@app.route('/ppwr/declarations', methods=['GET'])
def ppwr_declarations_page():
    try:
        resp = fastapi_list_supplier_declarations()
        declarations = resp.get('declarations', []) if resp and resp.get('success') else []

        # Fetch latest assessments for materials present in the declarations list
        material_ids = sorted({d.get('material_id') for d in declarations if d.get('material_id')})
        assessments_by_mat = {}
        for mid in material_ids:
            try:
                aresp = fastapi_get_assessments(mid)
                if aresp and aresp.get('success'):
                    items = aresp.get('assessments') or []
                    if items:
                        assessments_by_mat[mid] = items[0]
            except Exception:
                app.logger.exception(f"Failed to fetch assessments for {mid}")

        return render_template(
            'ppwr_declarations.html',
            declarations=declarations,
            eval_result=None,
            evaluated_id=None,
            assessments=assessments_by_mat,
        )
    except Exception as e:
        app.logger.error(f"PPWR declarations page error: {e}", exc_info=True)
        flash('Failed to fetch declarations from backend', 'danger')
        return render_template('ppwr_declarations.html', declarations=[], eval_result=None, evaluated_id=None, assessments={})

@app.route('/ppwr/declarations/evaluate', methods=['POST'])
def ppwr_declarations_evaluate():
    try:
        decl_id = request.form.get('decl_id')
        material_id = request.form.get('material_id')
        if not decl_id or not material_id:
            flash('Missing declaration id or material id', 'danger')
            return redirect(url_for('ppwr_declarations_page'))

        # Run assessment in backend using stored declaration
        result = fastapi_assess_from_declaration(int(decl_id), material_id)
        if not result or not result.get('success'):
            flash('Assessment failed', 'danger')
            # Still show list
            resp = fastapi_list_supplier_declarations()
            declarations = resp.get('declarations', []) if resp and resp.get('success') else []
            # Reuse existing assessments map to keep badges populated
            material_ids = sorted({d.get('material_id') for d in declarations if d.get('material_id')})
            assessments_by_mat = {}
            for mid in material_ids:
                try:
                    aresp = fastapi_get_assessments(mid)
                    if aresp and aresp.get('success'):
                        items = aresp.get('assessments') or []
                        if items:
                            assessments_by_mat[mid] = items[0]
                except Exception:
                    app.logger.exception(f"Failed to fetch assessments for {mid}")

            return render_template(
                'ppwr_declarations.html',
                declarations=declarations,
                eval_result=None,
                evaluated_id=None,
                assessments=assessments_by_mat,
            )

        assessment = result.get('assessment')
        if not assessment:
            flash('No assessment generated', 'warning')

        # Refresh list and render with inline result for this decl
        resp = fastapi_list_supplier_declarations(material_id=material_id)
        declarations = resp.get('declarations', []) if resp and resp.get('success') else []
        # Populate assessments so the status badges/inline blocks render after a POST
        material_ids = sorted({d.get('material_id') for d in declarations if d.get('material_id')})
        assessments_by_mat = {}
        for mid in material_ids:
            try:
                aresp = fastapi_get_assessments(mid)
                if aresp and aresp.get('success'):
                    items = aresp.get('assessments') or []
                    if items:
                        assessments_by_mat[mid] = items[0]
            except Exception:
                app.logger.exception(f"Failed to fetch assessments for {mid}")

        return render_template(
            'ppwr_declarations.html',
            declarations=declarations,
            eval_result=assessment,
            evaluated_id=int(decl_id),
            assessments=assessments_by_mat,
        )
    except Exception as e:
        app.logger.error(f"PPWR evaluate error: {e}", exc_info=True)
        flash('Error during evaluation', 'danger')
        return redirect(url_for('ppwr_declarations_page'))


@app.route('/ppwr/declarations/evaluate-all', methods=['POST'])
def ppwr_declarations_evaluate_all():
    """Run PPWR assessment for every listed declaration when explicitly requested.

    This prevents automatic runs on page load and lets the user trigger batch evaluation.
    """
    try:
        resp = fastapi_list_supplier_declarations()
        declarations = resp.get('declarations', []) if resp and resp.get('success') else []
        successes = 0
        failures = []
        for d in declarations:
            decl_id = d.get('id') or d.get('decl_id')
            material_id = d.get('material_id')
            if not decl_id or not material_id:
                failures.append({'id': decl_id, 'material_id': material_id, 'error': 'missing ids'})
                continue
            try:
                result = fastapi_assess_from_declaration(int(decl_id), str(material_id))
                if result and result.get('success'):
                    successes += 1
                else:
                    failures.append({'id': decl_id, 'material_id': material_id, 'error': (result or {}).get('error', 'unknown')})
            except Exception as ex:
                app.logger.exception(f"Batch PPWR evaluate failed for decl {decl_id}")
                failures.append({'id': decl_id, 'material_id': material_id, 'error': str(ex)})

        # Rebuild assessments map so UI shows latest results
        material_ids = sorted({d.get('material_id') for d in declarations if d.get('material_id')})
        assessments_by_mat = {}
        for mid in material_ids:
            try:
                aresp = fastapi_get_assessments(mid)
                if aresp and aresp.get('success'):
                    items = aresp.get('assessments') or []
                    if items:
                        assessments_by_mat[mid] = items[0]
            except Exception:
                app.logger.exception(f"Failed to fetch assessments for {mid}")

        if successes:
            flash(f"Ran PPWR assessment for {successes} declaration(s)", 'success')
        if failures:
            flash(f"{len(failures)} declaration(s) failed to evaluate", 'warning')

        return render_template(
            'ppwr_declarations.html',
            declarations=declarations,
            eval_result=None,
            evaluated_id=None,
            assessments=assessments_by_mat,
        )
    except Exception as e:
        app.logger.error(f"PPWR evaluate-all error: {e}", exc_info=True)
        flash('Error during batch evaluation', 'danger')
        return redirect(url_for('ppwr_declarations_page'))


@app.route('/api/ppwr/assessments/batch', methods=['POST'])
def api_ppwr_assessments_batch():
    """Return latest PPWR assessments for a list of material IDs.

    Body: { material_ids: ["A123", "B234", ...] }
    Response: { success: bool, assessments: { material_id: {..} } }
    """
    try:
        payload = request.get_json(silent=True) or {}
        material_ids = payload.get('material_ids') or payload.get('materials') or []
        if not material_ids:
            return jsonify({'success': False, 'error': 'material_ids required'}), 400

        out = {}
        for mid in material_ids:
            mid_str = str(mid).strip()
            if not mid_str:
                continue
            try:
                aresp = fastapi_get_assessments(mid_str)
                if aresp and aresp.get('success'):
                    items = aresp.get('assessments') or []
                    if items:
                        out[mid_str] = items[0]
            except Exception:
                app.logger.exception(f"Failed to fetch assessment for {mid_str}")

        return jsonify({'success': True, 'assessments': out})
    except Exception as e:
        app.logger.error(f"PPWR batch assessments error: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/ppwr/assess-from-declaration', methods=['POST'])
def api_ppwr_assess_from_declaration():
    """Proxy to backend to assess PPWR from a stored declaration.

    Expects form or JSON with keys: decl_id, material_id.
    Returns JSON: { success: bool, assessment: {..}, pipeline_result: {..} }
    """
    try:
        decl_id = None
        material_id = None
        if request.is_json:
            payload = request.get_json(silent=True) or {}
            decl_id = payload.get('decl_id')
            material_id = payload.get('material_id')
        else:
            decl_id = request.form.get('decl_id')
            material_id = request.form.get('material_id')

        if not decl_id or not material_id:
            return jsonify({'success': False, 'error': 'Missing decl_id or material_id'}), 400

        try:
            decl_id_int = int(str(decl_id).strip())
        except Exception:
            return jsonify({'success': False, 'error': 'Invalid decl_id'}), 400

        result = fastapi_assess_from_declaration(decl_id_int, str(material_id).strip())
        if result and result.get('success'):
            return jsonify(result)
        return jsonify({'success': False, 'error': 'Backend assessment failed', 'result': result or {}}), 500
    except Exception as e:
        app.logger.error(f"PPWR API assess-from-declaration error: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)}), 500

# UI: Map declarations to material IDs and trigger assessment
@app.route('/ppwr/map-declarations', methods=['GET'])
def ppwr_map_declarations():
    try:
        resp = fastapi_list_supplier_declarations()
        declarations = resp.get('declarations', []) if resp and resp.get('success') else []
    except Exception:
        declarations = []
    return render_template('ppwr_map_declarations.html', declarations=declarations, result=None)

@app.route('/ppwr/map-declarations/run', methods=['POST'])
def ppwr_map_declarations_run():
    decl_id = request.form.get('decl_id')
    material_id = request.form.get('material_id')
    apply_to_duplicates = request.form.get('apply_to_duplicates')
    scope = request.form.get('scope') or 'sku'
    if not decl_id or not material_id:
        flash('Missing declaration id or material id', 'danger')
        return redirect(url_for('ppwr_map_declarations'))
    try:
        decl_id_int = int(str(decl_id).strip())
        # First, map declaration to material and optionally fan-out
        try:
            r_map = requests.post(f"{os.environ.get('PPWR_BACKEND_URL', 'http://127.0.0.1:8001')}/ppwr/supplier-declarations/map",
                                  data={'decl_id': decl_id_int, 'material_id': material_id, 'apply_to_duplicates': apply_to_duplicates or 'false', 'scope': scope}, timeout=20)
            map_resp = r_map.json()
        except Exception as e:
            map_resp = {'success': False, 'error': str(e)}
        # Then run assessment
        result = fastapi_assess_from_declaration(decl_id_int, str(material_id).strip())
        # If assessment was triggered, set ppwr_flag for affected BOM rows to avoid re-ingestion
        try:
            if result and isinstance(result, dict):
                success = result.get('success')
                if success:
                    q = PFASBOM.query.filter_by(material=str(material_id).strip())
                    if scope == 'sku' and decl_id_int:
                        # if declaration had a SKU, narrow to that
                        if 'declaration' in map_resp and isinstance(map_resp['declaration'], dict) and map_resp['declaration'].get('sku'):
                            q = q.filter_by(sku=map_resp['declaration']['sku'])
                    rows = q.all()
                    for r in rows:
                        r.ppwr_flag = True
                        db.session.add(r)
                    db.session.commit()
        except Exception:
            db.session.rollback()
            app.logger.exception('Failed to set ppwr_flag after PPWR assessment')
        # Merge mapping response into result for display context
        if isinstance(result, dict) and isinstance(map_resp, dict):
            result.update({k: v for k, v in map_resp.items() if k not in result})
    except Exception as e:
        result = {'success': False, 'error': str(e)}
    # Re-list declarations and show result inline
    try:
        resp = fastapi_list_supplier_declarations(material_id=material_id)
        declarations = resp.get('declarations', []) if resp and resp.get('success') else []
    except Exception:
        declarations = []
    return render_template('ppwr_map_declarations.html', declarations=declarations, result=result)

# Debug: verify declaration storage path and backend index presence
@app.route('/api/debug/ppwr/storage-index', methods=['GET'])
def api_debug_ppwr_storage_index():
    """Return stored path for declaration and Chroma index status if available.

    Params: material_id, decl_id
    """
    try:
        material_id = request.args.get('material_id') or ''
        decl_id = request.args.get('decl_id') or ''
        out = {
            'success': True,
            'material_id': material_id,
            'decl_id': decl_id,
            'frontend': {},
            'backend': {},
        }
        # Frontend storage details
        try:
            drec = None
            if decl_id:
                try:
                    drec = SupplierDeclaration.query.filter_by(id=int(decl_id)).first()
                except Exception:
                    drec = None
            if not drec and material_id:
                drec = SupplierDeclaration.query.filter_by(material=material_id).order_by(SupplierDeclaration.id.desc()).first()
            if drec:
                out['frontend']['stored_path'] = getattr(drec, 'stored_path', None)
                out['frontend']['sku'] = getattr(drec, 'sku', None)
                out['frontend']['supplier_name'] = getattr(drec, 'supplier_name', None)
                # Count links for this material
                try:
                    cnt = db.session.query(PPWRMaterialDeclarationLink).filter_by(material_id=material_id).count()
                    out['frontend']['links_count'] = cnt
                except Exception:
                    out['frontend']['links_count'] = None
        except Exception as e:
            out['frontend']['error'] = str(e)

        # Backend declarations list and optional chroma chunks
        try:
            blist = fastapi_list_supplier_declarations(material_id=material_id)
            out['backend']['declarations'] = blist
        except Exception as e:
            out['backend']['declarations_error'] = str(e)
        # Attempt chroma chunks endpoint if available
        try:
            base = os.environ.get('FASTAPI_BASE_URL') or 'http://127.0.0.1:8000'
            import requests as _rq
            rr = _rq.get(f"{base}/ppwr/chroma/chunks", params={'material_id': material_id, 'decl_id': decl_id}, timeout=10)
            if rr.status_code == 200:
                out['backend']['chroma'] = rr.json()
            else:
                out['backend']['chroma_status'] = rr.status_code
        except Exception as e:
            out['backend']['chroma_error'] = str(e)
        return jsonify(out)
    except Exception as e:
        app.logger.exception('debug storage-index error')
        return jsonify({'success': False, 'error': str(e)}), 500

# Debug helper: list sample materials and a declaration id if available
@app.route('/api/debug/ppwr/list-materials', methods=['GET'])
def api_debug_ppwr_list_materials():
    try:
        # List distinct materials from BOM
        mats = []
        try:
            rows = db.session.query(PPWRBOM.material_id).distinct().all()
            mats = [r[0] for r in rows if r and r[0]]
        except Exception as e:
            app.logger.exception('Failed to list BOM materials')
        # Try to fetch one declaration id from backend
        decl_id = None
        try:
            blist = fastapi_list_supplier_declarations()
            if blist and blist.get('declarations'):
                d0 = blist['declarations'][0]
                decl_id = d0.get('id') or d0.get('decl_id')
        except Exception:
            pass
        return jsonify({'success': True, 'materials': mats[:50], 'sample_decl_id': decl_id})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/ppwr/declarations/upload', methods=['POST'])
def ppwr_declarations_upload_proxy():
    try:
        file = request.files.get('file')
        if not file:
            flash('No file uploaded', 'danger')
            return redirect(url_for('ppwr_declarations_page'))
        fname = file.filename or 'document.bin'
        tmp_dir = os.path.join(app.root_path, 'tmp_uploads')
        os.makedirs(tmp_dir, exist_ok=True)
        tmp_path = os.path.join(tmp_dir, fname)
        file.save(tmp_path)

        sku = request.form.get('sku') or ''
        material = request.form.get('material') or ''
        supplier_name = request.form.get('supplier_name') or ''
        description = request.form.get('description') or ''

        # Infer material_id from filename prefix when missing, and enforce mismatch checks
        inferred_id = None
        try:
            base = os.path.splitext(fname)[0]
            tokens = [t for t in re.split(r'[\s_\-]+', base) if t]
            if tokens:
                cand = tokens[0]
                if re.match(r'^[A-Za-z0-9_\-]{1,40}$', cand):
                    inferred_id = cand
        except Exception:
            inferred_id = None

        # If both provided and inferred exist but mismatch, reject
        if material and inferred_id and material != inferred_id:
            app.logger.warning(f"PPWR proxy: material mismatch — form={material} filename_id={inferred_id} fname={fname}")
            flash(f"Material mismatch: form '{material}' vs filename '{inferred_id}'. Upload rejected.", 'danger')
            try:
                os.remove(tmp_path)
            except Exception:
                pass
            return redirect(url_for('ppwr_declarations_page'))

        # If material missing, use inferred
        if not material and inferred_id:
            material = inferred_id

        # Validate BOM existence for SKU+material; if missing, log/warn but continue upload
        try:
            bom_exists = True
            if sku and material:
                exists = db.session.query(PPWRBOM).filter_by(sku=sku, material_id=material).first()
                if not exists:
                    bom_exists = False
                    app.logger.warning(f"PPWR proxy: no PPWR BOM match for sku={sku} material={material}; proceeding with upload")
                    flash(f"Warning: No PPWR BOM row found for SKU '{sku}' and material '{material}'. Upload will proceed.", 'warning')
        except Exception:
            bom_exists = False
            app.logger.exception("PPWR proxy: BOM validation error; proceeding with upload")
            flash("Warning: BOM validation error. Upload will proceed.", 'warning')

        app.logger.info(f"PPWR proxy: sending file to backend sku={sku} material={material} supplier={supplier_name} description={description} tmp_path={tmp_path}")
        resp = fastapi_upload_supplier_declaration(
            file_path=tmp_path,
            sku=sku,
            material_id=material,
            supplier_name=supplier_name,
            description=description,
        )
        try:
            app.logger.info(f"PPWR proxy: backend response: {resp}")
        except Exception:
            pass
        try:
            os.remove(tmp_path)
        except Exception:
            pass

    except Exception as e:
        app.logger.error(f"PPWR proxy: exception {e}", exc_info=True)
        flash(f"Error: {e}", 'danger')
        try:
            if 'tmp_path' in locals():
                os.remove(tmp_path)
        except Exception:
            pass
    return redirect(url_for('ppwr_declarations_page'))


# Browser -> Frontend proxy: map declaration to material and persist in frontend DB,
# then forward mapping to backend FastAPI so both sides stay in sync.
@app.route('/api/ppwr/supplier-declarations/map', methods=['POST'])
def api_ppwr_map_supplier_declaration():
    try:
        # Accept form or json
        if request.is_json:
            payload = request.get_json(silent=True) or {}
            decl_id = payload.get('decl_id')
            material_id = payload.get('material_id')
            apply_to_duplicates = payload.get('apply_to_duplicates', False)
            scope = payload.get('scope', 'sku')
        else:
            decl_id = request.form.get('decl_id')
            material_id = request.form.get('material_id')
            apply_to_duplicates = request.form.get('apply_to_duplicates', 'false')
            scope = request.form.get('scope', 'sku')

        if not decl_id or not material_id:
            return jsonify({'success': False, 'error': 'Missing decl_id or material_id'}), 400

        try:
            decl_id_int = int(str(decl_id).strip())
        except Exception:
            return jsonify({'success': False, 'error': 'Invalid decl_id'}), 400

        mat = str(material_id).strip()
        do_fan = str(apply_to_duplicates).strip().lower() in {'true','1','yes','on'}

        local_links_created = 0
        # Persist in frontend DB
        try:
            # Update declaration record if present
            decl = SupplierDeclaration.query.filter_by(id=decl_id_int).first()
            if decl:
                decl.material = mat
                db.session.add(decl)
                db.session.flush()

            if do_fan:
                bom_q = PPWRBOM.query.filter_by(material_id=mat)
                if scope == 'sku' and decl and decl.sku:
                    bom_q = bom_q.filter_by(sku=decl.sku)
                bom_rows = bom_q.all()
                for r in bom_rows:
                    link = PPWRMaterialDeclarationLink(material_id=mat, decl_id=decl_id_int, sku=r.sku)
                    db.session.add(link)
                    local_links_created += 1

            # If not doing fan-out and decl exists, still create a single link record
            if not do_fan and decl:
                link = PPWRMaterialDeclarationLink(material_id=mat, decl_id=decl_id_int, sku=decl.sku)
                db.session.add(link)
                local_links_created += 1

            db.session.commit()
        except Exception as e:
            db.session.rollback()
            app.logger.exception(f"Frontend mapping DB error: {e}")

        # Forward mapping request to backend FastAPI (best-effort).
        # If the backend reports the declaration does not exist (404-like response),
        # try to upload the declaration bytes we have locally to the backend and
        # then retry the mapping using the backend declaration id.
        backend_resp = None
        try:
            backend_resp = fastapi_map_supplier_declaration(decl_id_int, mat, do_fan, scope)
            # If backend responds but says declaration not found, attempt to push the file bytes
            if backend_resp and isinstance(backend_resp, dict) and not backend_resp.get('success') and backend_resp.get('error') and 'not found' in str(backend_resp.get('error')).lower():
                app.logger.info(f"Backend mapping reported declaration missing for decl_id={decl_id_int}; attempting backend upload then retry mapping")
                # Try to find local declaration bytes and forward them to backend upload
                try:
                    if decl and getattr(decl, 'file_data', None):
                        # write to temp file and call backend upload helper
                        import tempfile, os
                        tf = None
                        try:
                            # Use a filename that includes the material token so backend filename-inference accepts it
                            prefix = f"{mat}_" if mat else ''
                            suffix = '.pdf' if (getattr(decl, 'document_type', '') == 'pdf') else '.txt'
                            tf = tempfile.NamedTemporaryFile(delete=False, prefix=prefix, suffix=suffix)
                            tf.write(decl.file_data)
                            tf.flush()
                            tf.close()
                            backend_upload_resp = fastapi_upload_supplier_declaration(file_path=tf.name, sku=decl.sku, material_id=mat, supplier_name=decl.supplier_name, description=decl.description)
                            if backend_upload_resp and isinstance(backend_upload_resp, dict):
                                # backend may return id under various keys depending on endpoint shape
                                backend_id = None
                                if backend_upload_resp.get('id'):
                                    backend_id = backend_upload_resp.get('id')
                                elif backend_upload_resp.get('uploaded') and isinstance(backend_upload_resp.get('uploaded'), list) and backend_upload_resp.get('uploaded'):
                                    backend_id = backend_upload_resp.get('uploaded')[0].get('id')
                                elif backend_upload_resp.get('success') and backend_upload_resp.get('backend') and backend_upload_resp.get('backend').get('id'):
                                    backend_id = backend_upload_resp.get('backend').get('id')

                                if backend_id:
                                    app.logger.info(f"Uploaded declaration to backend with id={backend_id}; retrying mapping")
                                    backend_resp = fastapi_map_supplier_declaration(int(backend_id), mat, do_fan, scope)
                                else:
                                    app.logger.warning(f"Backend upload returned no id: {backend_upload_resp}")
                        finally:
                            try:
                                if tf:
                                    os.unlink(tf.name)
                            except Exception:
                                pass
                except Exception as e:
                    app.logger.exception(f"Failed to upload declaration to backend during mapping retry: {e}")
        except Exception as e:
            app.logger.exception(f"Forward to backend mapping failed: {e}")

        # Best-effort: mark ppwr_flag on BOM rows when mapping succeeds and fan-out is applied
        try:
            q = PPWRBOM.query.filter_by(material_id=mat)
            if scope == 'sku' and isinstance(backend_resp, dict):
                # Try to derive SKU either from frontend decl or backend response
                sku_val = None
                try:
                    sku_val = decl.sku if decl else None
                except Exception:
                    sku_val = None
                if not sku_val:
                    try:
                        if backend_resp.get('declaration') and backend_resp['declaration'].get('sku'):
                            sku_val = backend_resp['declaration']['sku']
                    except Exception:
                        sku_val = None
                if sku_val:
                    q = q.filter_by(sku=sku_val)
            rows = q.all()
            for r in rows:
                r.ppwr_flag = True
                db.session.add(r)
            db.session.commit()
        except Exception:
            db.session.rollback()
            app.logger.exception('Failed to set ppwr_flag during mapping')

        result = {
            'success': True,
            'decl_id': decl_id_int,
            'material_id': mat,
            'frontend_links_created': local_links_created,
            'backend_response': backend_resp
        }
        return jsonify(result)
    except Exception as e:
        app.logger.exception(f"api_ppwr_map_supplier_declaration error: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500
    
@app.route('/filter')
def filter():
    return render_template('filterpage.html')

@app.route('/bom-uploads')
def bom_uploads_page():
    """Render a dedicated BOM Uploads page with drag/drop and client-side validation."""
    # Deprecated: redirect to dashboard; the standalone BOM upload page is removed.
    return redirect(url_for('index'))


# Removed: separate Supplier Declarations page in favor of in-assessment uploads


@app.route('/api/bom/upload', methods=['POST'])
def api_bom_upload():
    """API endpoint to accept BOM uploads, validate schema, store BOM rows and write audit entries.

    Expects multipart/form-data with 'file' (CSV or Excel) and optional 'assessments' form values.
    Returns JSON { success: bool, inserted: int, updated: int, skipped: int, errors: [...] }
    """
    try:
        file = request.files.get('file')
        if not file:
            return jsonify({'success': False, 'error': 'No file uploaded'}), 400

        filename = file.filename.lower()
        if filename.endswith('.csv'):
            df = pd.read_csv(file.stream, dtype=str)
        elif filename.endswith(('.xlsx', '.xls')):
            df = pd.read_excel(file.stream, sheet_name=0, dtype=str)
        else:
            return jsonify({'success': False, 'error': 'Unsupported file type'}), 400

        # Normalize column names
        df.columns = [str(col).strip().replace('\n','').replace('\r','').replace(' ', '_').lower() for col in df.columns]

        # Accept a wider variety of header name variants (normalized to lower case, spaces -> underscores)
        required_cols = {
            'sku': [
                'sku', 'product_id', 'part_number',
                'sku_#', 'sku#', 'sku_number', 'sku_number'
            ],
            'product': [
                'product', 'product_name', 'product_desc', 'product_description'
            ],
            'component': [
                'component', 'comp', 'component_id',
                'component_#', 'component#', 'component_number', 'component_number'
            ],
            'component_description': [
                'component_description', 'comp_desc', 'component_desc', 'component description'
            ],
            'subcomponent': [
                'subcomponent', 'sub_comp', 'subcomponent_id',
                'sub_component_#', 'sub_component#', 'sub_component', 'sub component'
            ],
            'subcomponent_description': [
                'subcomponent_description', 'sub_comp_desc', 'subcomponent_desc', 'sub_component_description', 'sub component description'
            ],
            'material': [
                'material', 'mat_id', 'material_id',
                'material_#', 'material#', 'material_number', 'material_number'
            ],
            'material_name': [
                'material_name', 'mat_name', 'material_desc', 'material_description'
            ]
        }

        # Optionally accept chemical data columns in the BOM so we can persist them
        optional_cols = {
            'cas_number': ['cas', 'cas_number', 'cas_no', 'cas#'],
            'chemical_name': ['chemical_name', 'chem_name', 'chemical', 'substance', 'chemical name'],
            'concentration_ppm': ['concentration_ppm', 'concentration', 'conc_ppm', 'conc', 'ppm'],
            'supplier_name': ['supplier', 'supplier_name', 'supplier name', 'vendor', 'vendor_name'],
            'reference_doc': ['reference', 'reference_doc', 'reference_document', 'ref']
        }

        col_map = {}
        for target, possibles in required_cols.items():
            found = next((c for c in possibles if c in df.columns), None)
            if not found:
                return jsonify({'success': False, 'error': f'Missing required column: {target}'}), 400
            col_map[target] = found

        # If both a human-friendly 'material' column and an ID-like material column exist,
        # prefer the ID-like column for the internal material key so joins with `result` succeed.
        id_candidates = ['material_id', 'mat_id', 'material_code', 'material_no', 'material#', 'material_number', 'materialnumber', 'materialcode', 'matcode']
        for c in id_candidates:
            if c in df.columns:
                # override the material mapping to use ID-like column
                col_map['material'] = c
                break

        # Map optional columns if present
        optional_map = {}
        for target, possibles in optional_cols.items():
            found = next((c for c in possibles if c in df.columns), None)
            optional_map[target] = found
        app.logger.info(f"BOM API upload: column mapping: {col_map}")
        app.logger.info(f"BOM API upload: optional columns detected: {optional_map}")

        inserted = updated = skipped = 0
        audit_entries = []
        commit_time = datetime.utcnow()

        for index, row in df.iterrows():
            try:
                sku_val = str(row[col_map['sku']]).strip().split('.')[0]
                comp_val = str(row[col_map['component']]).strip().split('.')[0]
                subcomp_val = str(row[col_map['subcomponent']]).strip()

                # Normalize material value (handles multi-line cells like "Name\nID")
                material_raw = row[col_map['material']]
                material_val = _normalize_material_value(material_raw).split('.')[0]

                # If material is empty, try to derive from material_name column
                if not material_val and col_map.get('material_name'):
                    material_val = _normalize_material_value(row[col_map['material_name']]).split('.')[0]

                # Support multiple materials in one cell (comma/semicolon/pipe/slash separated)
                material_list = [m.strip() for m in re.split(r'[;,|/]+', material_val) if m.strip()] if material_val else []
                if not material_list:
                    # nothing we can do for this row
                    skipped += 1
                    audit_entries.append({'sku': sku_val, 'material': material_val, 'action': 'skip', 'details': 'missing primary key'})
                    continue

                # For each material in the (possibly split) list, insert/update BOM row
                for material_val_single in material_list:
                    material_val_single = material_val_single.split('.')[0]
                    try:
                        existing = db.session.query(PFASBOM).filter_by(
                            sku=sku_val, component=comp_val, subcomponent=subcomp_val, material=material_val_single
                        ).first()

                        if existing:
                            existing.product = str(row[col_map['product']]).strip()
                            # Update non-key fields
                            existing.component_description = str(row[col_map['component_description']]).strip() if col_map.get('component_description') and col_map.get('component_description') in row else existing.component_description
                            existing.subcomponent_description = str(row[col_map['subcomponent_description']]).strip() if col_map.get('subcomponent_description') and col_map.get('subcomponent_description') in row else existing.subcomponent_description
                            existing.material_name = str(row[col_map['material_name']]).strip() if col_map.get('material_name') and col_map.get('material_name') in row else existing.material_name
                            existing.uploaded_at = commit_time
                            updated += 1
                            action = 'update'
                        else:
                            bom_item = PFASBOM(
                                sku=sku_val,
                                material=material_val_single,
                                product=str(row[col_map['product']]).strip() if col_map.get('product') else None,
                                component=comp_val,
                                component_description=str(row[col_map['component_description']]).strip() if col_map.get('component_description') else None,
                                subcomponent=subcomp_val,
                                subcomponent_description=str(row[col_map['subcomponent_description']]).strip() if col_map.get('subcomponent_description') else None,
                                material_name=str(row[col_map['material_name']]).strip() if col_map.get('material_name') else None,
                                portal_name='SAP',
                                region='Global',
                                assessment=','.join(request.form.getlist('assessments')) or 'PFAS',
                                uploaded_at=commit_time
                            )
                            db.session.add(bom_item)
                            inserted += 1
                            action = 'insert'

                        # Append an audit entry per material
                        audit_entries.append({
                            'sku': sku_val,
                            'product': str(row[col_map['product']]).strip() if col_map.get('product') else None,
                            'component': comp_val,
                            'component_description': str(row[col_map['component_description']]).strip() if col_map.get('component_description') else None,
                            'subcomponent': subcomp_val,
                            'subcomponent_description': str(row[col_map['subcomponent_description']]).strip() if col_map.get('subcomponent_description') else None,
                            'material': material_val_single,
                            'material_name': str(row[col_map['material_name']]).strip() if col_map.get('material_name') else None,
                            'action': action,
                            'uploaded_at': commit_time.isoformat()
                        })
                    except Exception as e:
                        # per-material error
                        db.session.rollback()
                        skipped += 1
                        audit_entries.append({'sku': sku_val, 'material': material_val_single, 'action': 'error', 'details': str(e)})
                        continue

                # ---- Optional: persist chemical data if present in the BOM row ----
                try:
                    def _get(col_key):
                        name = optional_map.get(col_key)
                        if not name:
                            return None
                        val = row[name]
                        if pd.isna(val):
                            return None
                        s = str(val).strip()
                        return s if s != '' else None

                    # Read chemical fields once per-row
                    cas_val = _get('cas_number')
                    chem_val = _get('chemical_name')
                    supplier_val = _get('supplier_name')
                    ref_val = _get('reference_doc')

                    conc_val = None
                    conc_raw = _get('concentration_ppm')
                    if conc_raw is not None:
                        try:
                            # store as Decimal to match Numeric column
                            conc_val = Decimal(conc_raw)
                        except (InvalidOperation, ValueError):
                            try:
                                conc_val = Decimal(str(float(conc_raw)))
                            except Exception:
                                conc_val = None

                    # Only upsert if we have at least one chemical-related field
                    if any([cas_val, chem_val, conc_val is not None, supplier_val, ref_val]):
                        # Upsert per-material entry (material_val_single) — ensure the
                        # chemical row matches the exact material ID inserted above.
                        for material_val_single in material_list:
                            m_id = material_val_single
                            existing_mat = db.session.query(PFASMaterialChemicals).filter_by(material_id=m_id).first()
                            if existing_mat:
                                if cas_val:
                                    existing_mat.cas_number = cas_val
                                if chem_val:
                                    existing_mat.chemical_name = chem_val
                                if conc_val is not None:
                                    existing_mat.concentration_ppm = conc_val
                                if supplier_val:
                                    existing_mat.supplier_name = supplier_val
                                if ref_val:
                                    existing_mat.reference_doc = ref_val
                            else:
                                new_mat = PFASMaterialChemicals(
                                    material_id=m_id,
                                    cas_number=cas_val,
                                    material_name=str(row[col_map['material_name']]).strip() if col_map.get('material_name') else None,
                                    chemical_name=chem_val,
                                    concentration_ppm=conc_val,
                                    supplier_name=supplier_val,
                                    reference_doc=ref_val
                                )
                                db.session.add(new_mat)

                            # mark BOM row(s) as having chemical data
                            try:
                                bom_rows = db.session.query(PFASBOM).filter_by(sku=sku_val, component=comp_val, subcomponent=subcomp_val, material=m_id).all()
                                for bom_row in bom_rows:
                                    bom_row.flag = True
                            except Exception:
                                pass

                        # Log what we wrote/updated for this material row for debugging
                        try:
                            app.logger.info(f"BOM API upload: upserted chemical for materials={material_list} cas={cas_val} chem={chem_val} conc={conc_val} supplier={supplier_val} ref={ref_val}")
                        except Exception:
                            pass
                except Exception:
                    # Non-fatal: don't break entire upload if chemical upsert fails
                    app.logger.debug(f"Optional chemical upsert failed for row {index}", exc_info=True)

            except Exception as e:
                db.session.rollback()
                skipped += 1
                audit_entries.append({'sku': None, 'material': None, 'action': 'error', 'details': str(e)})

        db.session.commit()

        # Persist audit rows
        try:
            for a in audit_entries:
                audit = PFASBOMAudit(
                    sku=a.get('sku'),
                    product=a.get('product'),
                    component=a.get('component'),
                    component_description=a.get('component_description'),
                    subcomponent=a.get('subcomponent'),
                    subcomponent_description=a.get('subcomponent_description'),
                    material=a.get('material'),
                    material_name=a.get('material_name'),
                    action=a.get('action'),
                    details=a.get('details', ''),
                    uploaded_at=commit_time
                )
                db.session.add(audit)
            db.session.commit()
        except Exception as e:
            db.session.rollback()
            app.logger.warning(f"Failed to write audit rows: {e}")

        return jsonify({'success': True, 'inserted': inserted, 'updated': updated, 'skipped': skipped, 'uploaded_at': commit_time.isoformat()}), 200

    except Exception as e:
        app.logger.error(f"BOM upload API failure: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/upload', methods=['POST'])
def upload_bom():
    app.logger.info("Starting BOM upload")
    # Ensure any previous aborted transaction is cleared before processing
    try:
        db.session.rollback()
    except Exception:
        pass

    file = request.files.get('file')
    if not file:
        flash("No file uploaded.", "danger")
        return redirect(url_for('index'))
    
    regions = ",".join(request.form.getlist('regions')) or "Global"
    # Normalize common 'all' region variants to 'Global'
    try:
        rnorm = regions.strip().lower()
        if rnorm in ('all', 'all region', 'all regions', 'all-region', 'all-regions'):
            regions = 'Global'
    except Exception:
        pass
    assessments = ",".join(request.form.getlist('assessments')) or "PFAS"
    portal_name = "SAP"
    
    try:
        # If a PDF/document is uploaded in the BOM Upload, do NOT forward anymore.
        # Show a clear message to use the PPWR Assessment row Upload for supplier declarations.
        filename = file.filename.lower()
        _, _ext = os.path.splitext(filename)
        if _ext in ALLOWED_DECL_EXT and _ext == '.pdf':
            app.logger.info(f"PDF detected in BOM upload; advising user to use Assessment Upload instead: {filename}")
            flash("Unsupported file for BOM: Please upload CSV/Excel for BOM. Use the PPWR Assessment 'Upload' button per row to add supplier declarations.", "warning")
            return redirect(url_for('index'))

         # Normal BOM parsing path (CSV / Excel)
        file_stream = file.stream.read()
        file_buffer = BytesIO(file_stream)

        if filename.endswith('.csv'):
            df = pd.read_csv(file_buffer, dtype=str)
        elif filename.endswith(('.xlsx', '.xls')):
            df = pd.read_excel(file_buffer, sheet_name=0, dtype=str)
        else:
            flash("Unsupported BOM file type. Please upload CSV or Excel.", "danger")
            return redirect(url_for('index'))
        
        # Normalize columns
        df.columns = [
            str(col).strip().replace('\n', '').replace('\r', '').replace(' ', '_').lower() 
            for col in df.columns
        ]
        
        # Accept a wider variety of header name variants (normalized to lower case, spaces -> underscores)
        required_cols = {
            'sku': [
                'sku', 'product_id', 'part_number',
                'sku_#', 'sku#', 'sku_number', 'sku_number'
            ],
            'product': [
                'product', 'product_name', 'product_desc', 'product_description'
            ],
            'component': [
                'component', 'comp', 'component_id',
                'component_#', 'component#', 'component_number', 'component_number'
            ],
            'component_description': [
                'component_description', 'comp_desc', 'component_desc', 'component description'
            ],
            'subcomponent': [
                'subcomponent', 'sub_comp', 'subcomponent_id',
                'sub_component_#', 'sub_component#', 'sub_component', 'sub component'
            ],
            'subcomponent_description': [
                'subcomponent_description', 'sub_comp_desc', 'subcomponent_desc', 'sub_component_description', 'sub component description'
            ],
            'material': [
                'material', 'mat_id', 'material_id',
                'material_#', 'material#', 'material_number', 'material_number'
            ],
            'material_name': [
                'material_name', 'mat_name', 'material_desc', 'material_description'
            ]
        }
        
        col_map = {}
        for target, possible_names in required_cols.items():
            found = next((name for name in possible_names if name in df.columns), None)
            if not found:
                flash(f"Missing required column: {target}", "danger")
                return redirect(url_for('index'))
            col_map[target] = found

        # If both a human-friendly 'material' column and an ID-like material column exist,
        # prefer the ID-like column for the internal material key so joins with `result` succeed.
        id_candidates = ['material_id', 'mat_id', 'material_code', 'material_no', 'material#', 'material_number', 'materialnumber', 'materialcode', 'matcode']
        for c in id_candidates:
            if c in df.columns:
                col_map['material'] = c
                break

        # Map optional columns if present
        optional_cols = {
            'cas_number': ['cas', 'cas_number', 'cas_no', 'cas#'],
            'chemical_name': ['chemical_name', 'chem_name', 'chemical', 'substance', 'chemical name'],
            'concentration_ppm': ['concentration_ppm', 'concentration', 'conc_ppm', 'conc', 'ppm'],
            'supplier_name': ['supplier', 'supplier_name', 'supplier name', 'vendor', 'vendor_name'],
            'reference_doc': ['reference', 'reference_doc', 'reference_document', 'ref']
        }
        optional_map = {k: next((c for c in v if c in df.columns), None) for k, v in optional_cols.items()}
        app.logger.info(f"BOM upload (UI): column mapping: {col_map}")
        app.logger.info(f"BOM upload (UI): optional columns detected: {optional_map}")

        inserted, updated, skipped = 0, 0, 0

        # single upload timestamp for this operation (UTC)
        commit_time = datetime.utcnow()
        ts_str = commit_time.strftime("%Y-%m-%d %H:%M:%S UTC")

        # Collect SKUs affected by this upload
        affected_skus = set()

        for index, row in df.iterrows():
            try:
                # Process ALL composite key fields first
                sku_val = str(row[col_map['sku']]).strip().split('.')[0]
                component_val = str(row[col_map['component']]).strip().split('.')[0]
                subcomponent_val = str(row[col_map['subcomponent']]).strip()
                # Normalize material value (handles multi-line cells like "Name\nID")
                material_raw = row[col_map['material']]
                material_val = _normalize_material_value(material_raw).split('.')[0]

                # If material is empty, try to derive from material_name column
                if not material_val and col_map.get('material_name'):
                    material_val = _normalize_material_value(row[col_map['material_name']]).split('.')[0]

                # Support multiple materials in one cell (comma/semicolon/pipe/slash separated)
                material_list = [m.strip() for m in re.split(r'[;,|/]+', material_val) if m.strip()] if material_val else []

                product_val = str(row[col_map['product']]).strip()

                # Validate basic composite key presence
                if not (sku_val and component_val and subcomponent_val and material_list):
                    skipped += 1
                    app.logger.warning(f"Row {index} skipped due to missing or invalid primary key values: SKU={sku_val}, Component={component_val}, Subcomponent={subcomponent_val}, Material={material_val}")
                    continue

                affected_skus.add(sku_val)

                # Check if BOM already exists using COMPOSITE PRIMARY KEY
                try:
                    existing = db.session.query(PFASBOM).filter_by(
                        sku=sku_val,
                        component=component_val,
                        subcomponent=subcomponent_val,
                        material=material_val
                    ).first()
                except (sqlalchemy.exc.InternalError, sqlalchemy.exc.ProgrammingError) as qerr:
                    # If a prior error aborted the transaction, rollback and skip this row
                    try:
                        db.session.rollback()
                    except Exception:
                        pass
                    app.logger.error(f"DB query failed for row {index}, skipping: {qerr}", exc_info=True)
                    skipped += 1
                    continue

                if existing:
                    # Update existing record (non-key fields only)
                    existing.product = product_val
                    existing.component_description = str(row[col_map['component_description']]).strip()
                    existing.subcomponent_description = str(row[col_map['subcomponent_description']]).strip()
                    existing.material_name = str(row[col_map['material_name']]).strip()
                    existing.portal_name = portal_name
                    existing.region = regions
                    existing.assessment = assessments
                    # Persist upload timestamp for dashboard BOM_time
                    try:
                        existing.uploaded_at = commit_time
                    except Exception:
                        pass
                    updated += 1
                else:
                    # Insert new record per material in the list
                    for material_val_single in material_list:
                        try:
                            bom_item = PFASBOM(
                                sku=sku_val,
                                material=material_val_single,
                                product=product_val,
                                component=component_val,  # Use pre-processed value
                                component_description=str(row[col_map['component_description']]).strip(),
                                subcomponent=subcomponent_val,  # Use pre-processed value
                                subcomponent_description=str(row[col_map['subcomponent_description']]).strip(),
                                material_name=str(row[col_map['material_name']]).strip(),
                                portal_name=portal_name,
                                region=regions,
                                assessment=assessments,
                                uploaded_at=commit_time
                            )
                            db.session.add(bom_item)
                            inserted += 1
                        except Exception as e:
                            db.session.rollback()
                            skipped += 1
                            app.logger.error(f"Failed to insert BOM row for SKU={sku_val} material={material_val_single}: {e}", exc_info=True)
                            continue

                # ---- Optional: persist chemical data if present in the BOM row ----
                try:
                    def _get(col_key):
                        name = optional_map.get(col_key)
                        if not name:
                            return None
                        val = row[name]
                        if pd.isna(val):
                            return None
                        s = str(val).strip()
                        return s if s != '' else None

                    cas_val = _get('cas_number')
                    chem_val = _get('chemical_name')
                    supplier_val = _get('supplier_name')
                    ref_val = _get('reference_doc')

                    conc_val = None
                    conc_raw = _get('concentration_ppm')
                    if conc_raw is not None:
                        try:
                            conc_val = Decimal(conc_raw)
                        except (InvalidOperation, ValueError):
                            try:
                                conc_val = Decimal(str(float(conc_raw)))
                            except Exception:
                                conc_val = None

                    if any([cas_val, chem_val, conc_val is not None, supplier_val, ref_val]):
                        # Apply chemical info to each material in this row (handles multi-material cells)
                        materials_to_apply = material_list if material_list else [material_val]
                        for mat_id in materials_to_apply:
                            existing_mat = db.session.query(PFASMaterialChemicals).filter_by(material_id=mat_id).first()
                            if existing_mat:
                                if cas_val:
                                    existing_mat.cas_number = cas_val
                                if chem_val:
                                    existing_mat.chemical_name = chem_val
                                if conc_val is not None:
                                    existing_mat.concentration_ppm = conc_val
                                if supplier_val:
                                    existing_mat.supplier_name = supplier_val
                                if ref_val:
                                    existing_mat.reference_doc = ref_val
                            else:
                                new_mat = PFASMaterialChemicals(
                                    material_id=mat_id,
                                    cas_number=cas_val,
                                    material_name=str(row[col_map['material_name']]).strip() if col_map.get('material_name') else None,
                                    chemical_name=chem_val,
                                    concentration_ppm=conc_val,
                                    supplier_name=supplier_val,
                                    reference_doc=ref_val
                                )
                                db.session.add(new_mat)

                            # Mark BOM flag true for this material row
                            try:
                                bom_row = db.session.query(PFASBOM).filter_by(sku=sku_val, component=component_val, subcomponent=subcomponent_val, material=mat_id).first()
                                if bom_row:
                                    bom_row.flag = True
                            except Exception:
                                pass

                            # Log per-material change
                            try:
                                app.logger.info(f"BOM upload (UI): upserted chemical for material={mat_id} cas={cas_val} chem={chem_val} conc={conc_val} supplier={supplier_val} ref={ref_val}")
                            except Exception:
                                pass
                except Exception:
                    app.logger.debug(f"Optional chemical upsert failed for row {index}", exc_info=True)

            except Exception as e:
                # If any per-row processing error occurs, rollback the session to clear
                # the failed transaction so subsequent rows can continue processing.
                try:
                    db.session.rollback()
                except Exception:
                    pass
                app.logger.error(f"Row {index} error: {e}", exc_info=True)
                skipped += 1
        
        db.session.commit()

        # Log upload timestamp and SKUs in backend log
        app.logger.info(f"BOM upload completed at {ts_str} - inserted: {inserted}, updated: {updated}, skipped: {skipped} - SKUs: {sorted(list(affected_skus))}")

        # Store last upload info in session so index() can display the timestamp in the table
        session['last_upload'] = {'timestamp': ts_str, 'skus': list(affected_skus)}

        # Flash success message including the required marker and timestamp
        flash(f"BOM_UPLOADED_SUCCESSFULLY: {inserted} inserted, {updated} updated, {skipped} skipped. Uploaded At: {ts_str}", "success")
        
    except Exception as e:
        db.session.rollback()
        app.logger.error(f"Upload failed: {e}", exc_info=True)
        flash("❌ Error processing file.", "danger")
    
    return redirect(url_for('index'))


@app.route('/debug-material/<material_id>')
def debug_material(material_id):
    """Debug endpoint: return the stored chemical row for a material_id (from `result` table).

    Use this to confirm whether the BOM upload wrote a `result` row for the given material id.
    """
    try:
        app.logger.info(f"Debug material lookup for: {material_id}")
        mat = db.session.query(PFASMaterialChemicals).filter_by(material_id=material_id).first()
        if not mat:
            return jsonify({'found': False, 'material_id': material_id}), 200

        conc = None
        try:
            conc = float(mat.concentration_ppm) if mat.concentration_ppm is not None else None
        except Exception:
            conc = str(mat.concentration_ppm) if mat.concentration_ppm is not None else None

        return jsonify({
            'found': True,
            'material_id': mat.material_id,
            'material_name': mat.material_name,
            'cas_number': mat.cas_number,
            'chemical_name': mat.chemical_name,
            'concentration_ppm': conc,
            'supplier_name': mat.supplier_name,
            'reference_doc': mat.reference_doc
        }), 200
    except Exception as e:
        app.logger.error(f"Debug material error: {e}", exc_info=True)
        return jsonify({'error': str(e)}), 500


@app.route('/debug-bom/<sku>')
def debug_bom(sku):
    """Return BOM rows for a SKU (material and material_name) to help diagnose join mismatches."""
    try:
        rows = db.session.query(PFASBOM).filter_by(sku=sku).all()
        if not rows:
            return jsonify({'found': False, 'sku': sku}), 200

        data = []
        for r in rows:
            data.append({
                'sku': r.sku,
                'component': r.component,
                'subcomponent': r.subcomponent,
                'material': r.material,
                'material_name': r.material_name,
                'flag': bool(r.flag)
            })

        return jsonify({'found': True, 'sku': sku, 'rows': data}), 200
    except Exception as e:
        app.logger.error(f"Debug BOM error: {e}", exc_info=True)
        return jsonify({'error': str(e)}), 500


@app.route('/upload-supplier-declaration', methods=['POST'])
def upload_supplier_declaration_frontend():
    """Accept a supplier declaration file and store it in Postgres (same DB as BOM).

    Expects multipart/form-data with:
      - file: the uploaded file
      - sku: (optional) product SKU to attach
      - material: (optional) material id to attach
      - metadata: (optional) JSON string with extra metadata
    Returns JSON with success and created metadata.
    """
    try:
        f = request.files.get('file')
        if not f:
            return jsonify({'success': False, 'error': 'No file uploaded'}), 400

        fname = f.filename
        if not _allowed_decl(fname):
            return jsonify({'success': False, 'error': 'Unsupported file extension'}), 400

        sku = request.form.get('sku')
        material = request.form.get('material')

        # If material is not provided, infer from filename prefix (ID-like token)
        if not material:
            try:
                base = os.path.splitext(fname)[0]
                tokens = [t for t in re.split(r'[\s_\-]+', base) if t]
                if tokens:
                    cand = tokens[0]
                    if re.match(r'^[A-Za-z0-9_\-]{1,40}$', cand):
                        material = cand
            except Exception:
                pass

        # Parse optional metadata JSON
        metadata = None
        metadata_raw = request.form.get('metadata')
        if metadata_raw:
            try:
                metadata = json.loads(metadata_raw)
            except Exception:
                metadata = None

        # Read file bytes for DB storage
        file_bytes = f.read()
        file_size = len(file_bytes) if file_bytes is not None else None
        # Derive simple document type
        _ext = os.path.splitext(fname.lower())[1]
        if _ext in ('.pdf',):
            doc_type = 'pdf'
        elif _ext in ('.docx', '.doc'):
            doc_type = 'docx'
        elif _ext in ('.xlsx', '.xls'):
            doc_type = 'xlsx'
        elif _ext in ('.txt',):
            doc_type = 'txt'
        elif _ext in ('.csv',):
            doc_type = 'csv'
        else:
            doc_type = None

        # Create ORM entity for current SupplierDeclaration model
        # Validate BOM existence to prevent unmatched material IDs
        try:
            if material:
                if sku:
                    exists = db.session.query(PFASBOM).filter_by(sku=sku, material=material).first()
                else:
                    exists = db.session.query(PFASBOM).filter_by(material=material).first()
                if not exists:
                    return jsonify({'success': False, 'error': f"No BOM row found for material '{material}'"}), 400
        except Exception as _e:
            return jsonify({'success': False, 'error': 'BOM validation failed'}), 500

        decl = SupplierDeclaration(
            sku=sku,
            material=material,
            original_filename=fname,
            storage_filename=None,
            file_path=None,
            document_type=doc_type,
            upload_date=datetime.utcnow(),
            metadata_json=metadata,
            file_size=file_size,
            file_data=file_bytes,
        )

        db.session.add(decl)
        try:
            db.session.commit()
        except Exception as commit_err:
            db.session.rollback()
            # Fallback: some legacy schemas enforce NOT NULL on columns like
            # document_type/original_filename/storage_filename/file_path.
            # Insert via raw SQL including those fields with benign defaults.
            try:
                # Fallback insert supporting legacy columns
                sql = text(
                    """
                    INSERT INTO supplier_declarations
                        (sku, material, original_filename, storage_filename, file_path, document_type, upload_date,
                         metadata_json, file_size, file_data)
                    VALUES
                        (:sku, :material, :original_filename, :storage_filename, :file_path, :document_type, :upload_date,
                         CAST(:metadata_json AS JSON), :file_size, :file_data)
                    RETURNING id
                    """
                )
                params = {
                    'sku': sku,
                    'material': material,
                    'original_filename': fname,
                    'storage_filename': None,
                    'file_path': None,
                    'document_type': doc_type,
                    'upload_date': datetime.utcnow(),
                    'metadata_json': json.dumps(metadata) if isinstance(metadata, (dict, list)) else None,
                    'file_size': file_size,
                    'file_data': file_bytes,
                }
                res = db.session.execute(sql, params)
                row = res.fetchone()
                db.session.commit()
                new_id = row[0] if row else None
                return jsonify({
                    'success': True,
                    'id': new_id,
                    'filename': fname,
                    'sku': sku,
                    'material': material,
                    'file_size': file_size
                }), 201
            except Exception as e2:
                app.logger.error(f"Supplier declaration upload fallback insert failed: {e2}", exc_info=True)
                raise commit_err

        return jsonify({
            'success': True,
            'id': decl.id,
            'filename': decl.original_filename,
            'sku': decl.sku,
            'material': decl.material,
            'file_size': decl.file_size
        }), 201

    except Exception as e:
        try:
            db.session.rollback()
        except Exception:
            pass
        app.logger.error(f"Supplier declaration upload failed: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/manual-upsert-chemical', methods=['POST'])
def manual_upsert_chemical():
    """Admin helper: manually insert or update a PFASMaterialChemicals row.

    Accepts JSON body with keys:
      - material_id (required)
      - cas_number
      - chemical_name
      - concentration_ppm (number)
      - supplier_name
      - reference_doc

    This is intended as a small admin/debug helper to populate the `result` table
    when automated ingestion is not available. It performs an upsert and returns
    the created/updated row.
    """
    try:
        payload = request.get_json(force=True)
        if not payload:
            return jsonify({'success': False, 'error': 'Missing JSON body'}), 400

        material_id = payload.get('material_id')
        if not material_id:
            return jsonify({'success': False, 'error': 'material_id is required'}), 400

        cas = payload.get('cas_number')
        chem = payload.get('chemical_name')
        conc = payload.get('concentration_ppm')
        supplier = payload.get('supplier_name')
        ref = payload.get('reference_doc')

        # Normalize concentration if provided
        conc_val = None
        if conc is not None:
            try:
                from decimal import Decimal
                conc_val = Decimal(str(conc))
            except Exception:
                conc_val = None

        # Upsert into PFASMaterialChemicals
        existing = db.session.query(PFASMaterialChemicals).filter_by(material_id=material_id).first()
        if existing:
            if cas:
                existing.cas_number = cas
            if chem:
                existing.chemical_name = chem
            if conc_val is not None:
                existing.concentration_ppm = conc_val
            if supplier:
                existing.supplier_name = supplier
            if ref:
                existing.reference_doc = ref
            action = 'updated'
        else:
            new_row = PFASMaterialChemicals(
                material_id=material_id,
                cas_number=cas,
                material_name=payload.get('material_name'),
                chemical_name=chem,
                concentration_ppm=conc_val,
                supplier_name=supplier,
                reference_doc=ref
            )
            db.session.add(new_row)
            action = 'inserted'

        db.session.commit()

        return jsonify({'success': True, 'action': action, 'material_id': material_id}), 200
    except Exception as e:
        db.session.rollback()
        app.logger.error(f"manual_upsert_chemical failed: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/assessment-upload', methods=['POST'])
def api_assessment_upload():
    """Compatibility endpoint for the assessment UI upload buttons.

    Re-uses the existing supplier-declaration upload logic so the UI can POST
    files to /api/assessment-upload and receive the same JSON response.
    """
    app.logger.info("/api/assessment-upload called")
    # Forward to existing handler which expects multipart/form-data
    return upload_supplier_declaration_frontend()


@app.route('/supplier-declaration/<int:decl_id>/download')
def download_supplier_declaration(decl_id):
    try:
        decl = db.session.query(SupplierDeclaration).filter_by(id=decl_id).first()
        if not decl:
            return jsonify({'error': 'Not found'}), 404

        if getattr(decl, 'file_data', None):
            return send_file(
                BytesIO(decl.file_data),
                as_attachment=True,
                download_name=getattr(decl, 'original_filename', 'document.bin'),
                mimetype='application/pdf' if getattr(decl, 'document_type', None) == 'pdf' else 'application/octet-stream'
            )

        # Legacy disk path fallback
        file_path = getattr(decl, 'file_path', None)
        if file_path and os.path.exists(file_path):
            return send_file(file_path, as_attachment=True, download_name=getattr(decl, 'original_filename', 'document.bin'))

        return jsonify({'error': 'File missing'}), 404
    except Exception as e:
        app.logger.error(f"Download failed for declaration {decl_id}: {e}", exc_info=True)
        return jsonify({'error': str(e)}), 500


@app.route('/api/supplier-declarations/<sku>')
def list_supplier_declarations(sku):
    try:
        include_archived = request.args.get('include_archived', '0') in ('1', 'true', 'True')
        # Use fields defined on the current `SupplierDeclaration` model in models.py
        q = db.session.query(SupplierDeclaration).filter_by(sku=sku)
        if not include_archived:
            # Filter out archived if the column exists
            try:
                q = q.filter(SupplierDeclaration.is_archived == False)  # noqa: E712
            except Exception:
                pass
        rows = q.order_by(SupplierDeclaration.upload_date.desc()).all()

        items = []
        for r in rows:
            # Provide both legacy keys and UI-expected keys for compatibility
            uploaded_iso = r.upload_date.isoformat() if getattr(r, 'upload_date', None) else None
            items.append({
                # Core
                'id': r.id,
                'sku': r.sku,
                'material': getattr(r, 'material', None),
                'file_size': getattr(r, 'file_size', None),
                'metadata': getattr(r, 'metadata_json', None),
                'is_archived': getattr(r, 'is_archived', False),
                # Current UI expects these keys
                'original_filename': getattr(r, 'original_filename', None),
                'document_type': getattr(r, 'document_type', None),
                'upload_date': uploaded_iso,
                'supplier_name': getattr(r, 'supplier_name', None),
                'description': getattr(r, 'description', None),
                # Back-compat keys still used elsewhere
                'filename': getattr(r, 'original_filename', None),
                'stored_path': getattr(r, 'file_path', None),
                'content_type': getattr(r, 'document_type', None),
                'uploaded_at': uploaded_iso,
            })

        # Return under 'declarations' key for the frontend JS to consume
        return jsonify({'success': True, 'declarations': items}), 200
    except Exception as e:
        app.logger.error(f"List supplier declarations failed: {e}", exc_info=True)
        return jsonify({'error': str(e)}), 500


# Strict mapping status: join declarations to BOM by sku+material
@app.route('/api/document-mapping/<sku>', methods=['GET'])
def api_document_mapping(sku):
    try:
        # Fetch declarations for the SKU
        decls = db.session.query(SupplierDeclaration).filter_by(sku=sku).all()
        # Build BOM material set for the SKU
        bom_rows = db.session.query(PFASBOM.material).filter_by(sku=sku).all()
        bom_set = set([r.material for r in bom_rows])

        items = []
        for d in decls:
            mat = getattr(d, 'material', None)
            is_mapped = bool(mat) and mat in bom_set
            items.append({
                'id': d.id,
                'filename': getattr(d, 'original_filename', None),
                'document_type': getattr(d, 'document_type', None),
                'material': mat,
                'is_mapped': is_mapped,
                'mapped_to': mat if is_mapped else None,
                'upload_date': d.upload_date.isoformat() if getattr(d, 'upload_date', None) else None
            })
        return jsonify({'success': True, 'sku': sku, 'mappings': items}), 200
    except Exception as e:
        app.logger.error(f"document-mapping failed: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)}), 500


# New: UI-compatible upload endpoint supporting multiple files
@app.route('/api/supplier-declarations/upload', methods=['POST'])
def api_supplier_declarations_upload():
    """Accept multiple supplier declaration files and store them in Postgres.

    Form fields:
      - files: one or more files
      - sku: required target SKU
      - material: optional material id
      - supplier_name: optional supplier name
      - description: optional description
    """
    try:
        sku = request.form.get('sku')
        if not sku:
            return jsonify({'error': 'Missing sku'}), 400

        material = request.form.get('material')
        supplier_name = request.form.get('supplier_name')
        description = request.form.get('description')

        files = request.files.getlist('files')
        if not files:
            return jsonify({'error': 'No files uploaded'}), 400

        uploaded = []
        commit_time = datetime.utcnow()
        errors = []
        for f in files:
            try:
                fname = f.filename
                if not _allowed_decl(fname):
                    errors.append({'filename': fname, 'error': 'Unsupported file extension'})
                    continue

                # Infer material from filename if not provided
                mat_val = material or ''
                if not mat_val:
                    try:
                        base = os.path.splitext(fname)[0]
                        tokens = [t for t in re.split(r'[\s_\-]+', base) if t]
                        if tokens:
                            cand = tokens[0]
                            if re.match(r'^[A-Za-z0-9_\-]{1,40}$', cand):
                                mat_val = cand
                    except Exception:
                        mat_val = ''

                # Validate BOM existence for SKU + material
                if not mat_val:
                    errors.append({'filename': fname, 'error': 'Material could not be inferred from filename'})
                    continue
                try:
                    exists = db.session.query(PFASBOM).filter_by(sku=sku, material=mat_val).first()
                    if not exists:
                        errors.append({'filename': fname, 'error': f"No BOM row for SKU '{sku}' and material '{mat_val}'"})
                        continue
                except Exception as _e:
                    errors.append({'filename': fname, 'error': 'BOM validation failed'})
                    continue

                file_bytes = f.read()
                file_size = len(file_bytes) if file_bytes is not None else None
                _ext = os.path.splitext(fname.lower())[1]
                if _ext in ('.pdf',):
                    doc_type = 'pdf'
                elif _ext in ('.docx', '.doc'):
                    doc_type = 'docx'
                elif _ext in ('.xlsx', '.xls'):
                    doc_type = 'xlsx'
                elif _ext in ('.txt',):
                    doc_type = 'txt'
                elif _ext in ('.csv',):
                    doc_type = 'csv'
                else:
                    doc_type = None

                decl = SupplierDeclaration(
                    sku=sku,
                    material=mat_val,
                    original_filename=fname,
                    storage_filename=None,
                    file_path=None,
                    document_type=doc_type,
                    supplier_name=supplier_name,
                    description=description,
                    upload_date=commit_time,
                    file_size=file_size,
                    file_data=file_bytes,
                )
                db.session.add(decl)
                db.session.flush()  # get id
                uploaded.append({
                    'id': decl.id,
                    'original_filename': decl.original_filename,
                    'upload_date': decl.upload_date.isoformat() if decl.upload_date else None
                })
            except Exception as e:
                db.session.rollback()
                errors.append({'filename': f.filename if f else None, 'error': str(e)})
                # continue with next file
        # Try to commit all successful inserts
        try:
            db.session.commit()
        except Exception as e:
            db.session.rollback()
            return jsonify({'error': str(e), 'uploaded': uploaded, 'errors': errors, 'total_uploaded': 0, 'total_errors': len(files)}), 500

        status = 201 if uploaded else 400
        return jsonify({
            'success': True if uploaded else False,
            'uploaded': uploaded,
            'errors': errors,
            'total_uploaded': len(uploaded),
            'total_errors': len(errors),
            'upload_time': commit_time.isoformat() + 'Z'
        }), status
    except Exception as e:
        try:
            db.session.rollback()
        except Exception:
            pass
        return jsonify({'error': str(e)}), 500


# New: UI-compatible download endpoint
@app.route('/api/supplier-declarations/download/<int:decl_id>')
def api_supplier_declarations_download(decl_id):
    return download_supplier_declaration(decl_id)

# New: Proxy upload to backend PPWR storage (single file)
@app.route('/api/ppwr/supplier-declarations/upload', methods=['POST'])
def api_ppwr_supplier_declarations_upload():
    """Forward a single supplier declaration file to backend FastAPI for PPWR storage."""
    try:
        file = request.files.get('file')
        if not file:
            return jsonify({'error': 'No file uploaded'}), 400
        fname = file.filename or 'document.bin'
        tmp_dir = os.path.join(app.root_path, 'tmp_uploads')
        os.makedirs(tmp_dir, exist_ok=True)
        tmp_path = os.path.join(tmp_dir, fname)
        file.save(tmp_path)

        sku = request.form.get('sku')
        material = request.form.get('material')
        # Infer material from filename prefix if not provided (e.g., "B7462_Paperboard.pdf" -> "B7462")
        inferred_id = None
        try:
            base = os.path.splitext(fname)[0]
            tokens = [t for t in re.split(r'[\s_\-]+', base) if t]
            if tokens:
                cand = tokens[0]
                if re.match(r'^[A-Za-z0-9_\-]{1,40}$', cand):
                    inferred_id = cand
        except Exception:
            inferred_id = None

        # If both provided and inferred exist but mismatch, reject
        if material and inferred_id and material != inferred_id:
            app.logger.warning(f"PPWR API proxy: material mismatch — form={material} filename_id={inferred_id} fname={fname}")
            try:
                os.remove(tmp_path)
            except Exception:
                pass
            return jsonify({'success': False, 'error': 'Material mismatch with filename; upload rejected'}), 400

        if not material and inferred_id:
            material = inferred_id

        # Validate BOM existence before forwarding
        try:
            if sku and material:
                exists = db.session.query(PFASBOM).filter_by(sku=sku, material=material).first()
                if not exists:
                    app.logger.warning(f"PPWR API proxy: no BOM match for sku={sku} material={material}; rejecting upload")
                    try:
                        os.remove(tmp_path)
                    except Exception:
                        pass
                    return jsonify({'success': False, 'error': 'No BOM row for given SKU/material'}), 400
        except Exception:
            try:
                os.remove(tmp_path)
            except Exception:
                pass
            return jsonify({'success': False, 'error': 'BOM validation error'}), 500
        supplier_name = request.form.get('supplier_name')
        description = request.form.get('description')

        app.logger.info(f"PPWR API proxy: forwarding file to backend sku={sku} material={material} supplier={supplier_name} description={description} tmp_path={tmp_path}")
        resp = fastapi_upload_supplier_declaration(
            file_path=tmp_path,
            sku=sku,
            material_id=material,
            supplier_name=supplier_name,
            description=description,
        )
        try:
            app.logger.info(f"PPWR API proxy: backend response: {resp}")
        except Exception:
            pass
        try:
            os.remove(tmp_path)
        except Exception:
            pass

        if not resp:
            return jsonify({'success': False, 'error': 'Backend upload failed'}), 502
        return jsonify({'success': True, 'backend': resp}), 201
    except Exception as e:
        app.logger.error(f"PPWR API proxy: exception {e}", exc_info=True)
        return jsonify({'error': str(e)}), 500


# (Removed duplicate mapping route)


# New: UI-compatible delete endpoint
def _archive_declaration_by_id(decl_id: int, req_sku: str | None):
    """Set is_archived=true for the declaration, with optional SKU safety."""
    decl = db.session.query(SupplierDeclaration).filter_by(id=decl_id).first()
    if not decl:
        return False, 404, 'Not found'
    if req_sku and decl.sku != req_sku:
        return False, 400, 'SKU mismatch for archive request'
    # Try ORM update
    try:
        setattr(decl, 'is_archived', True)
        db.session.commit()
        return True, 200, 'Document archived'
    except Exception as e:
        app.logger.warning(f"ORM archive failed for decl_id={decl_id}: {e}. Falling back to raw SQL.")
        try:
            db.session.rollback()
        except Exception:
            pass
        # Raw SQL fallback
        try:
            if req_sku:
                result = db.session.execute(
                    text('UPDATE supplier_declarations SET is_archived = TRUE WHERE id = :id AND sku = :sku'),
                    {'id': decl_id, 'sku': req_sku}
                )
            else:
                result = db.session.execute(
                    text('UPDATE supplier_declarations SET is_archived = TRUE WHERE id = :id'),
                    {'id': decl_id}
                )
            db.session.commit()
            if hasattr(result, 'rowcount') and result.rowcount == 0:
                return False, 404, 'Not found or SKU mismatch'
            return True, 200, 'Document archived'
        except Exception as e2:
            try:
                db.session.rollback()
            except Exception:
                pass
            app.logger.exception(f"Raw SQL archive also failed for decl_id={decl_id}")
            return False, 500, str(e2)


@app.route('/api/supplier-declarations/<int:decl_id>', methods=['DELETE'])
def api_supplier_declarations_delete(decl_id):
    try:
        # Ensure we aren't in a failed transaction state from a prior error
        try:
            db.session.rollback()
        except Exception:
            pass

        # Optional safety: require sku match if provided by client
        req_sku = request.args.get('sku')

        # Soft-delete (archive) by default to avoid FK violations and keep audit trails
        ok, code, msg = _archive_declaration_by_id(decl_id, req_sku)
        if not ok:
            return jsonify({'error': msg}), code
        return jsonify({'success': True, 'message': msg}), 200
    except Exception as e:
        try:
            db.session.rollback()
        except Exception:
            pass
        app.logger.exception(f"Delete supplier declaration failed for id={decl_id}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/supplier-declarations/<int:decl_id>/restore', methods=['POST'])
def api_supplier_declarations_restore(decl_id):
    """Unarchive a supplier declaration (set is_archived=false). Accepts optional sku for safety."""
    try:
        try:
            db.session.rollback()
        except Exception:
            pass

        req_sku = request.args.get('sku')
        decl = db.session.query(SupplierDeclaration).filter_by(id=decl_id).first()
        if not decl:
            return jsonify({'error': 'Not found'}), 404
        if req_sku and decl.sku != req_sku:
            return jsonify({'error': 'SKU mismatch for restore request'}), 400

        try:
            setattr(decl, 'is_archived', False)
            db.session.commit()
            return jsonify({'success': True, 'message': 'Document restored'}), 200
        except Exception as e:
            app.logger.warning(f"ORM restore failed for decl_id={decl_id}: {e}. Falling back to raw SQL.")
            try:
                db.session.rollback()
            except Exception:
                pass
            try:
                if req_sku:
                    result = db.session.execute(
                        text('UPDATE supplier_declarations SET is_archived = FALSE WHERE id = :id AND sku = :sku'),
                        {'id': decl_id, 'sku': req_sku}
                    )
                else:
                    result = db.session.execute(
                        text('UPDATE supplier_declarations SET is_archived = FALSE WHERE id = :id'),
                        {'id': decl_id}
                    )
                db.session.commit()
                if hasattr(result, 'rowcount') and result.rowcount == 0:
                    return jsonify({'error': 'Not found or SKU mismatch'}), 404
                return jsonify({'success': True, 'message': 'Document restored'}), 200
            except Exception as e2:
                try:
                    db.session.rollback()
                except Exception:
                    pass
                app.logger.exception(f"Raw SQL restore also failed for decl_id={decl_id}")
                return jsonify({'error': str(e2)}), 500
    except Exception as e:
        try:
            db.session.rollback()
        except Exception:
            pass
        app.logger.exception(f"Restore supplier declaration failed for id={decl_id}")
        return jsonify({'error': str(e)}), 500


def pfas_assessment(sku):
    try:
        # Ensure any prior failed transaction is cleared
        try:
            db.session.rollback()
        except Exception:
            # ignore rollback errors
            pass

        # Step 1: Fetch ALL BOM entries for this SKU (base materials)
        # Some databases may have an older schema that lacks new columns (e.g., uploaded_at).
        # Querying the full model can fail with ProgrammingError in that case. Attempt a
        # normal model query first and fall back to selecting known existing columns.
        try:
            bom_entries = db.session.query(PFASBOM).filter_by(sku=sku).all()
        except (sqlalchemy.exc.ProgrammingError, sqlalchemy.exc.InternalError) as e:
            app.logger.warning(f"Model query failed, falling back to explicit column select: {e}")
            # Rollback the failed transaction before retrying
            try:
                db.session.rollback()
            except Exception:
                pass

            # Select explicit columns (avoid uploaded_at which may not exist)
            bom_entries = db.session.query(
                PFASBOM.sku,
                PFASBOM.component,
                PFASBOM.subcomponent,
                PFASBOM.material,
                PFASBOM.product,
                PFASBOM.component_description,
                PFASBOM.subcomponent_description,
                PFASBOM.material_name,
                PFASBOM.portal_name,
                PFASBOM.region,
                PFASBOM.assessment,
                PFASBOM.flag
            ).filter(PFASBOM.sku == sku).all()
        if not bom_entries:
            return jsonify({"error": "SKU not found"}), 404

        first_row = bom_entries[0]
        product_name = f"{first_row.sku}_{first_row.product}"

        # Step 2: Get all material IDs from BOM
        material_ids = [entry.material for entry in bom_entries]

        # Step 3: Perform LEFT JOIN to include materials even if no chemical data exists
        results = db.session.query(
            PFASBOM.component,
            PFASBOM.subcomponent,
            PFASBOM.material,
            PFASBOM.material_name,
            PFASMaterialChemicals.cas_number,
            PFASMaterialChemicals.chemical_name,
            PFASMaterialChemicals.concentration_ppm,
            PFASMaterialChemicals.supplier_name,
            PFASMaterialChemicals.reference_doc,
            PFASRegulations.australian_aics,
            PFASRegulations.australian_imap_tier_2,
            PFASRegulations.canadian_dsl,
            PFASRegulations.canada_pctsr_2012,
            PFASRegulations.eu_reach_pre_registered,
            PFASRegulations.eu_reach_registered_ppm,
            PFASRegulations.us_epa_tscainventory,
            PFASRegulations.us_epa_tsca12b
        ).outerjoin(PFASMaterialChemicals, PFASBOM.material == PFASMaterialChemicals.material_id) \
         .outerjoin(PFASRegulations, PFASMaterialChemicals.cas_number == PFASRegulations.cas_number) \
         .filter(PFASBOM.sku == sku).all()

        # Step 4: Define regulations
        regulations = [
            {'name': 'Australian AICS', 'col': 'australian_aics'},
            {'name': 'Australian IMAP Tier 2', 'col': 'australian_imap_tier_2'},
            {'name': 'Canadian DSL', 'col': 'canadian_dsl'},
            {'name': 'Canada PCTSR 2012', 'col': 'canada_pctsr_2012'},
            {'name': 'EU REACH Pre Registered', 'col': 'eu_reach_pre_registered'},
            {'name': 'EU REACH Registered', 'col': 'eu_reach_registered_ppm'},
            {'name': 'US EPA TSCA Inventory', 'col': 'us_epa_tscainventory'},
            {'name': 'US EPA TSCA 12B', 'col': 'us_epa_tsca12b'}
        ]

        data = []
        non_conforming_count = 0

        for row in results:
            # Use BOM data as fallback
            component = row.component or "-"
            subcomponent = row.subcomponent or "-"
            material = row.material or "-"
            material_name = row.material_name or "-"

            # Chemical data — use defaults if missing
            cas = row.cas_number or "Unknown"
            chem_name = row.chemical_name or "Unknown"

            # Handle concentration: if missing or invalid, set to None (will become "Unknown")
            try:
                conc = int(float(row.concentration_ppm) ) if row.concentration_ppm is not None else None
            except (ValueError, TypeError):
                conc = None

            supplier = row.supplier_name or "Unknown"

            # Check if this row has NO chemical data (i.e., left join produced nulls)
            has_chemical_data = row.cas_number is not None or row.chemical_name is not None

            limits = []
            is_non_conforming = False

            for reg in regulations:
                value = getattr(row, reg['col'], None)
                threshold = float(value) if value is not None else None

                if conc is None:
                    # 🚨 CRITICAL CHANGE: If concentration is unknown, mark as NON-CONFORMING for ALL regulations
                    status = 'exceeded'
                    color = 'danger'
                    limit_display = f"{threshold} ppm" if threshold is not None else "No Threshold"
                    is_non_conforming = True
                else:
                    # Only calculate compliance if we have a valid concentration
                    if threshold is None:
                        status = 'unknown'
                        color = 'warning'
                        limit_display = "No Data"
                    elif threshold < conc:
                        status = 'exceeded'
                        color = 'danger'
                        is_non_conforming = True
                        limit_display = f"{threshold} ppm"
                    else:
                        status = 'within'
                        color = 'success'
                        limit_display = f"{threshold} ppm"

                limits.append({
                    'name': reg['name'],
                    'limit': limit_display,
                    'status': status,
                    'color': color
                })

            if is_non_conforming:
                non_conforming_count += 1

            data.append({
                "component": component,
                "subcomponent": subcomponent,
                "material": material,
                "material_name": material_name,
                "supplier_name": supplier,
                "chemical_name": chem_name,
                "cas_number": cas,
                "concentration": f"{int(conc)} ppm" if conc is not None else "Unknown",
                "reference_doc": row.reference_doc or "—",
                "limits": limits,
                "status": "Non-Compliance" if is_non_conforming else ("No Chemical Data" if not has_chemical_data else "Compliance"),
                "status_color": "danger" if is_non_conforming else ("warning" if not has_chemical_data else "success")
            })

        in_conformance_count = len([d for d in data if d["status"] == "Compliance"])
        no_data_count = len([d for d in data if d["status"] == "No Chemical Data"])

        return jsonify({
            "product": product_name,
            "summary": {
                "total": len(data),
                "non_conforming": non_conforming_count,
                "in_conformance": in_conformance_count,
                "no_chemical_data": no_data_count
            },
            "data": data
        })

    except Exception as e:
        app.logger.error(f"Error in pfas_assessment for {sku}: {e}", exc_info=True)
        return jsonify({"error": "Failed to retrieve data"}), 500

def calculate_dynamic_summary(sku, assessment_data, strict: bool = False):
    """
    Calculate dynamic summary statistics based on regulatory thresholds.
    Checks each chemical against all regulatory limits from pfas_regulation table.
    """
    try:
        app.logger.info(f"🔍 Starting regulatory-based summary calculation for SKU: {sku}")
        
        # Get assessment data
        data_entries = assessment_data.get('data', [])
        app.logger.info(f"📋 Processing {len(data_entries)} assessment entries")
        
        # Calculate file progress metrics
        total_materials_query = db.session.query(PFASBOM.material).filter_by(sku=sku).distinct()
        total_materials = total_materials_query.count()
        
        # Count materials that have chemical data
        materials_with_chemicals_query = db.session.query(PFASBOM.material).filter_by(sku=sku).join(
            PFASMaterialChemicals, PFASBOM.material == PFASMaterialChemicals.material_id
        ).distinct()
        materials_with_chemicals = materials_with_chemicals_query.count()
        
        files_not_found = max(0, total_materials - materials_with_chemicals)
        
        app.logger.info(f"📁 Materials: Total={total_materials}, With Chemicals={materials_with_chemicals}, Not Found={files_not_found}")
        
        # Calculate conformance based on actual regulatory checks.
        # In strict mode (PPWR) unknown concentrations count as non-conforming.
        conformance_stats = calculate_regulatory_conformance(sku, strict=strict)

        # Calculate review metrics
        total_reviewed = len(data_entries)
        estimated_total = max(total_reviewed, total_materials) if total_reviewed > 0 else total_materials

        # Calculate alternative substance metrics based on conformance
        alt_found = max(0, conformance_stats['in_conformance'] // 3) if conformance_stats['in_conformance'] > 0 else 0
        alt_not_found = max(0, conformance_stats['non_conforming'] // 4) if conformance_stats['non_conforming'] > 0 else 0

        summary_stats = {
            'files': {
                'total': total_materials,
                'downloaded': materials_with_chemicals,
                'not_found': files_not_found,
                'progress_text': f"{materials_with_chemicals} / {total_materials}"
            },
            'review': {
                'reviewed': total_reviewed,
                'total_expected': estimated_total,
                'non_conforming': conformance_stats['non_conforming'],
                'in_conformance': conformance_stats['in_conformance'],
                'no_data': conformance_stats['no_chemical_data'],
                'alt_found': alt_found,
                'alt_not_found': alt_not_found,
                'progress_text': f"{total_reviewed} / {estimated_total}"
            }
        }

        app.logger.info(f"✅ Final regulatory-based summary stats: {summary_stats}")
        return summary_stats

    except Exception as e:
        app.logger.error(f"❌ Error calculating regulatory summary for {sku}: {e}", exc_info=True)
        return {
            'files': {
                'total': 0,
                'downloaded': 0,
                'not_found': 0,
                'progress_text': "Error / Error"
            },
            'review': {
                'reviewed': 0,
                'total_expected': 0,
                'non_conforming': 0,
                'in_conformance': 0,
                'no_data': 0,
                'alt_found': 0,
                'alt_not_found': 0,
                'progress_text': "Error / Error"
            }
        }


def calculate_regulatory_conformance(sku, strict: bool = False):
    """
    Calculate conformance/non-conformance based on actual regulatory thresholds.
    
    Returns:
        dict: Contains counts for non_conforming, in_conformance, no_chemical_data
    """
    try:
        app.logger.info(f"🏛️ Calculating regulatory conformance for SKU: {sku}")
        
        # Define regulatory columns to check
        regulatory_columns = [
            'australian_aics',
            'australian_imap_tier_2', 
            'canadian_dsl',
            'canada_pctsr_2012',
            'eu_reach_pre_registered',
            'eu_reach_registered_ppm',
            'us_epa_tscainventory',
            'us_epa_tsca12b'
        ]
        
        # Query to get all materials with their chemicals and regulatory data
        results = db.session.query(
            PFASBOM.material,
            PFASBOM.material_name,
            PFASMaterialChemicals.cas_number,
            PFASMaterialChemicals.chemical_name,
            PFASMaterialChemicals.concentration_ppm,
            PFASRegulations.australian_aics,
            PFASRegulations.australian_imap_tier_2,
            PFASRegulations.canadian_dsl,
            PFASRegulations.canada_pctsr_2012,
            PFASRegulations.eu_reach_pre_registered,
            PFASRegulations.eu_reach_registered_ppm,
            PFASRegulations.us_epa_tscainventory,
            PFASRegulations.us_epa_tsca12b
        ).outerjoin(
            PFASMaterialChemicals, PFASBOM.material == PFASMaterialChemicals.material_id
        ).outerjoin(
            PFASRegulations, PFASMaterialChemicals.cas_number == PFASRegulations.cas_number
        ).filter(PFASBOM.sku == sku).all()
        
        app.logger.info(f"📊 Found {len(results)} material-chemical-regulation records")
        
        # Counters
        non_conforming = 0
        in_conformance = 0
        no_chemical_data = 0
        
        # Track processed entries to avoid double counting
        processed_entries = set()
        
        for row in results:
            # Create unique identifier for this material-chemical combination
            entry_id = f"{row.material}_{row.cas_number}_{row.chemical_name}"
            
            if entry_id in processed_entries:
                continue  # Skip duplicates
            processed_entries.add(entry_id)
            
            # Check if this entry has chemical data
            has_chemical_data = (row.cas_number is not None and 
                               row.chemical_name is not None and
                               row.concentration_ppm is not None)
            
            if not has_chemical_data:
                no_chemical_data += 1
                app.logger.debug(f"📝 No chemical data for material {row.material}")
                continue
            
            # Parse concentration
            try:
                concentration = float(row.concentration_ppm) if row.concentration_ppm else 0.0
            except (ValueError, TypeError):
                concentration = 0.0
                app.logger.warning(f"⚠️ Invalid concentration for {row.cas_number}: {row.concentration_ppm}")
            
            # Check conformance against all regulatory thresholds
            is_non_conforming = False
            conformance_checks = 0
            
            for reg_col in regulatory_columns:
                threshold_value = getattr(row, reg_col, None)
                
                if threshold_value is not None:
                    conformance_checks += 1
                    threshold = float(threshold_value)
                    
                    # If concentration is unknown (0 or None): behavior depends on 'strict'
                    if concentration == 0.0 or concentration is None:
                        if strict:
                            is_non_conforming = True
                            app.logger.debug(f"❌ Non-conforming due to unknown concentration (strict mode): {row.chemical_name} ({row.cas_number})")
                            break
                        else:
                            # In legacy (non-strict) mode, treat as missing chemical data (counted elsewhere)
                            app.logger.debug(f"ℹ️ Unknown concentration treated as no_chemical_data (non-strict mode): {row.chemical_name} ({row.cas_number})")
                            # Stop checking further regulations for this entry
                            break
                    elif concentration > threshold:
                        is_non_conforming = True
                        app.logger.debug(f"❌ Non-conforming: {row.chemical_name} ({concentration} ppm > {threshold} ppm for {reg_col})")
                        break
            
            # Categorize the entry
            if is_non_conforming:
                non_conforming += 1
            elif conformance_checks > 0:  # Has regulatory data and passed all checks
                in_conformance += 1
                app.logger.debug(f"✅ Conforming: {row.chemical_name} passed all {conformance_checks} regulatory checks")
            else:
                # Has chemical data but no regulatory thresholds to compare against
                no_chemical_data += 1
                app.logger.debug(f"📝 No regulatory data for {row.chemical_name} ({row.cas_number})")
        
        # Handle materials with no chemical data at all
        materials_without_chemicals = db.session.query(PFASBOM.material).filter_by(sku=sku).filter(
            ~db.session.query(PFASMaterialChemicals.material_id).filter(
                PFASMaterialChemicals.material_id == PFASBOM.material
            ).exists()
        ).count()
        
        no_chemical_data += materials_without_chemicals
        
        conformance_result = {
            'non_conforming': non_conforming,
            'in_conformance': in_conformance, 
            'no_chemical_data': no_chemical_data
        }
        
        app.logger.info(f"🏛️ Regulatory conformance results: {conformance_result}")
        return conformance_result
        
    except Exception as e:
        app.logger.error(f"❌ Error calculating regulatory conformance: {e}", exc_info=True)
        return {
            'non_conforming': 0,
            'in_conformance': 0,
            'no_chemical_data': 0
        }
    
@app.route('/download-bom/<sku>')
def download_bom(sku):
    app.logger.info(f"Downloading BOM for SKU: {sku}")

    bom_data = db.session.query(PFASBOM).filter_by(sku=sku).all()

    if not bom_data:
        flash("No BOM data found for this SKU.", "danger")
        return redirect(url_for('index'))

    data = []
    for row in bom_data:
        data.append({
            'SKU': row.sku,
            'Product': row.product,
            'Component': row.component,
            'Component Description': row.component_description,
            'Sub-Component': row.subcomponent,
            'Sub-Component Description': row.subcomponent_description,
            'Material': row.material,
            'Material Name': row.material_name,
            'Portal Name': row.portal_name,
            'Region': row.region,
            'Assessment': row.assessment
        })

    df = pd.DataFrame(data)
    output = BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df.to_excel(writer, sheet_name='BOM Details', index=False)
    output.seek(0)

    filename = f"BOM_{sku}.xlsx"
    return send_file(
        output,
        as_attachment=True,
        download_name=filename,
        mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
    )


@app.route('/api/delete-product/<sku>', methods=['DELETE'])
def delete_product(sku):
    """Delete a product and all its BOM entries"""
    app.logger.info(f"Attempting to delete product with SKU: {sku}")
    
    try:
        # Find all BOM entries for this SKU
        bom_entries = db.session.query(PFASBOM).filter_by(sku=sku).all()
        
        if not bom_entries:
            app.logger.warning(f"No BOM entries found for SKU: {sku}")
            return jsonify({"error": "Product not found"}), 404
        
        # Get product name for response
        product_name = f"{bom_entries[0].sku}_{bom_entries[0].product}" if bom_entries else sku
        
        # Delete all BOM entries for this SKU
        deleted_count = db.session.query(PFASBOM).filter_by(sku=sku).delete()
        
        # Commit the deletion
        db.session.commit()
        
        app.logger.info(f"Successfully deleted {deleted_count} BOM entries for SKU: {sku}")
        
        return jsonify({
            "success": True,
            "message": f"Product '{product_name}' deleted successfully",
            "deleted_count": deleted_count,
            "sku": sku
        }), 200
        
    except Exception as e:
        db.session.rollback()
        app.logger.error(f"Error deleting product {sku}: {e}", exc_info=True)
        return jsonify({
            "error": "Failed to delete product. Please try again.",
            "details": str(e)
        }), 500

   
@app.route('/api/export-pfas-report/<sku>')
def export_pfas_report(sku):
    """
    Exports a comprehensive PFAS report for a single SKU.
    Includes BOM, chemical details, and regulatory limits with compliance status.
    """
    app.logger.info(f"📥 Generating PFAS report for SKU: {sku}")

    try:
        # Fetch BOM entries for the SKU
        bom_entries = db.session.query(PFASBOM).filter_by(sku=sku).all()
        if not bom_entries:
            return jsonify({"error": "SKU not found"}), 404

        product_name = bom_entries[0].product

        # Regulation columns
        regulation_columns = [
            'australian_aics',
            'australian_imap_tier_2',
            'canadian_dsl',
            'canada_pctsr_2012',
            'eu_reach_pre_registered',
            'eu_reach_registered_ppm',
            'us_epa_tscainventory',
            'us_epa_tsca12b'
        ]

        # Build the full query
        query = db.session.query(
            PFASBOM.sku,
            PFASBOM.product,
            PFASBOM.component,
            PFASBOM.component_description,
            PFASBOM.subcomponent,
            PFASBOM.subcomponent_description,
            PFASBOM.material,
            PFASBOM.material_name,
            PFASMaterialChemicals.cas_number,
            PFASMaterialChemicals.chemical_name,
            PFASMaterialChemicals.concentration_ppm,
            PFASMaterialChemicals.supplier_name,
            PFASMaterialChemicals.reference,
            PFASRegulations.australian_aics,
            PFASRegulations.australian_imap_tier_2,
            PFASRegulations.canadian_dsl,
            PFASRegulations.canada_pctsr_2012,
            PFASRegulations.eu_reach_pre_registered,
            PFASRegulations.eu_reach_registered_ppm,
            PFASRegulations.us_epa_tscainventory,
            PFASRegulations.us_epa_tsca12b
        ).join(PFASMaterialChemicals, PFASBOM.material == PFASMaterialChemicals.material_id)\
         .outerjoin(PFASRegulations, PFASMaterialChemicals.cas_number == PFASRegulations.cas_number)\
         .filter(PFASBOM.sku == sku)

        results = query.all()
        app.logger.info(f"📊 PFAS report query returned {len(results)} rows")

        if not results:
            return jsonify({"error": "No chemical data found for this SKU"}), 400

        # Prepare data for DataFrame
        rows = []
        for r in results:
            conc = int(float(r.concentration_ppm)) if r.concentration_ppm is not None else 0.0

            row = {
                "SKU": r.sku or "",
                "Product": r.product or "",
                "Component": r.component or "",
                "Component Description": r.component_description or "",
                "Sub-Component": r.subcomponent or "",
                "Sub-Component Description": r.subcomponent_description or "",
                "Material ID": r.material or "",
                "Material Name": r.material_name or "",
                "CAS Number": r.cas_number or "Unknown",
                "Chemical Name": r.chemical_name or "Unknown",
                "Chemical Concentration (ppm)": f"{int(conc)} ppm",
                "Supplier Name": r.supplier_name or "Unknown",
                "Reference": r.reference or ""
            }

            # Add regulation thresholds and compliance status
            for col in regulation_columns:
                value = getattr(r, col, None)
                if value is None:
                    row[col] = "No Data"
                else:
                    threshold = float(value)
                    status = "Non-Compliant" if conc > threshold else "Compliant"
                    row[col] = {
                        "value": threshold,
                        "status": "exceeded" if conc > threshold else "within"
                    }

            rows.append(row)

        # Create DataFrame
        df_data = []
        for row in rows:
            flat_row = {k: (v["value"] if isinstance(v, dict) else v) for k, v in row.items()}
            # Format ppm values
            if "Chemical Concentration (ppm)" not in flat_row:
                conc_val = row.get("Chemical Concentration (ppm)", "0.00 ppm")
                flat_row["Chemical Concentration (ppm)"] = conc_val
            df_data.append(flat_row)

        df = pd.DataFrame(df_data)

        # Write to Excel with styling
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='PFAS Assessment', index=False)
            worksheet = writer.sheets['PFAS Assessment']

            # Styling
            red_fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")  # Light red
            green_fill = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")  # Light green

            # Apply coloring for regulation columns
            start_row = 2
            col_idx_map = {col: df.columns.get_loc(col) + 1 for col in regulation_columns}
            for r_idx, row in enumerate(rows):
                for reg_col in regulation_columns:
                    if reg_col not in col_idx_map:
                        continue
                    cell = worksheet.cell(row=start_row + r_idx, column=col_idx_map[reg_col])
                    value_obj = row[reg_col]
                    if isinstance(value_obj, dict):
                        if value_obj["status"] == "exceeded":
                            cell.fill = red_fill
                        else:
                            cell.fill = green_fill

            # Auto-adjust column widths
            for i, col in enumerate(df.columns):
                max_len = max(df[col].astype(str).map(len).max(), len(col)) + 2
                worksheet.column_dimensions[chr(65 + i)].width = min(max_len, 50)

        output.seek(0)
        timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M")
        filename = f"PFAS_Report_{sku}_{timestamp}.xlsx"

        app.logger.info(f"✅ PFAS report generated: {filename}")

        return send_file(
            output,
            as_attachment=True,
            download_name=filename,
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )

    except Exception as e:
        app.logger.error(f"❌ Failed to generate PFAS report for {sku}: {e}", exc_info=True)
        return jsonify({"error": "Failed to generate report. Please try again."}), 500


@app.route("/download/pfas/<sku>")
def download_pfas_report(sku):
    try:
        app.logger.info(f"📥 Generating PFAS report for SKU={sku}")

        # Regulation columns to include
        regulation_columns = [
            #'australian_aics',
            #'australian_imap_tier_2',
            #'canadian_dsl',
            'canada_pctsr_2012',
            'eu_reach_pre_registered',
            'eu_reach_registered_ppm',
            'us_epa_tscainventory',
            'us_epa_tsca12b'
        ]

        # Build query: Join BOM → Chemicals → Regulations
        query = db.session.query(
            PFASBOM.sku,
            PFASBOM.product,
            PFASBOM.component,
            PFASBOM.component_description,
            PFASBOM.subcomponent,
            PFASBOM.subcomponent_description,
            PFASBOM.material,
            PFASBOM.material_name,
            PFASMaterialChemicals.cas_number,
            PFASMaterialChemicals.chemical_name,
            PFASMaterialChemicals.concentration_ppm,
            PFASMaterialChemicals.supplier_name,
            PFASMaterialChemicals.reference_doc,
            # Regulation thresholds
            PFASRegulations.australian_aics,
            PFASRegulations.australian_imap_tier_2,
            PFASRegulations.canadian_dsl,
            PFASRegulations.canada_pctsr_2012,
            PFASRegulations.eu_reach_pre_registered,
            PFASRegulations.eu_reach_registered_ppm,
            PFASRegulations.us_epa_tscainventory,
            PFASRegulations.us_epa_tsca12b
        ).join(PFASMaterialChemicals, PFASBOM.material == PFASMaterialChemicals.material_id) \
         .outerjoin(PFASRegulations, PFASMaterialChemicals.cas_number == PFASRegulations.cas_number) \
         .filter(PFASBOM.sku == sku)

        results = query.all()
        app.logger.info(f"📊 Query returned {len(results)} rows")

        if not results:
            return jsonify({"error": "No matching records found"}), 400

        # Prepare rows
        rows = []
        for r in results:
            conc = int(float(r.concentration_ppm)) if r.concentration_ppm is not None else 0.0

            row = {
                "SKU": r.sku or "",
                "Product": r.product or "",
                "Component": r.component or "",
                "Component Description": r.component_description or "",
                "Sub-Component": r.subcomponent or "",
                "Sub-Component Description": r.subcomponent_description or "",
                "Material ID": r.material or "",
                "Material Name": r.material_name or "",
                "CAS Number": r.cas_number or "Unknown",
                "Chemical Name": r.chemical_name or "Unknown",
                "Chemical Concentration": f"{int(conc)} ppm",
                "Supplier Name": r.supplier_name or "Unknown",
                "Reference": r.reference_doc or ""
            }

            # Add regulation values
            for col in regulation_columns:
                value = getattr(r, col, None)
                if value is None:
                    row[col] = "No Data"
                else:
                    threshold = float(value)
                    row[col] = {
                        "value": threshold,
                        "status": "exceeded" if conc > threshold else "within"
                    }

            rows.append(row)

        # Flatten for DataFrame
        df_data = []
        for row in rows:
            flat = {k: v for k, v in row.items() if not isinstance(v, dict)}
            for col in regulation_columns:
                if isinstance(row[col], dict):
                    flat[col] = f"{row[col]['value']} ppm"
                else:
                    flat[col] = row[col]
            df_data.append(flat)

        df = pd.DataFrame(df_data)

        # Write to Excel with coloring
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='PFAS Report', index=False)
            worksheet = writer.sheets['PFAS Report']

            # Coloring
            red_fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
            green_fill = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")

            start_row = 2
            for idx, row in enumerate(rows):
                for col_idx, col_name in enumerate(regulation_columns):
                    cell = worksheet.cell(row=start_row + idx, column=df.columns.get_loc(col_name) + 1)
                    value_obj = row[col_name]
                    if isinstance(value_obj, dict):
                        if value_obj["status"] == "exceeded":
                            cell.fill = red_fill
                        else:
                            cell.fill = green_fill

            # Auto-fit column width
            for i, col in enumerate(df.columns):
                max_len = max(df[col].astype(str).map(len).max(), len(col))
                worksheet.column_dimensions[chr(65 + i)].width = min(max_len + 2, 50)

        output.seek(0)
        now = pd.Timestamp.now().strftime("%Y%m%d_%H%M")
        filename = f"PFAS_Report_{sku}_{now}.xlsx"

        return send_file(
            output,
            as_attachment=True,
            download_name=filename,
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )

    except Exception as e:
        app.logger.error(f"❌ PFAS report export failed: {e}", exc_info=True)
        return jsonify({"error": "Export failed. Please try again."}), 500

# Add this test route to debug the summary calculation
@app.route('/test-summary/<sku>')
def test_summary(sku):
    """Test route to check summary calculation without full assessment"""
    try:
        app.logger.info(f"🧪 Testing summary calculation for SKU: {sku}")
        
        # Check BOM data
        bom_count = db.session.query(PFASBOM).filter_by(sku=sku).count()
        bom_materials = db.session.query(PFASBOM.material).filter_by(sku=sku).distinct().count()
        
        # Check chemical data
        chemical_count = db.session.query(PFASMaterialChemicals).join(
            PFASBOM, PFASMaterialChemicals.material_id == PFASBOM.material
        ).filter(PFASBOM.sku == sku).count()
        
        materials_with_chemicals = db.session.query(PFASBOM.material).filter_by(sku=sku).join(
            PFASMaterialChemicals, PFASBOM.material == PFASMaterialChemicals.material_id
        ).distinct().count()
        
        # Try to get PFAS assessment
        try:
            response = pfas_assessment(sku)
            pfas_data = response.get_json() if response.status_code == 200 else {"error": "Assessment failed"}
        except Exception as e:
            pfas_data = {"error": str(e)}
        
        # Try summary calculation
        try:
            if "error" not in pfas_data:
                # Use legacy (non-strict) behavior for PFAS summary by default
                summary_stats = calculate_dynamic_summary(sku, pfas_data, strict=False)
            else:
                summary_stats = {"error": "Could not calculate due to PFAS assessment failure"}
        except Exception as e:
            summary_stats = {"error": str(e)}
        
        return jsonify({
            "sku": sku,
            "bom_data": {
                "total_bom_records": bom_count,
                "unique_materials": bom_materials
            },
            "chemical_data": {
                "total_chemical_records": chemical_count,
                "materials_with_chemicals": materials_with_chemicals
            },
            "pfas_assessment": {
                "status": "success" if "error" not in pfas_data else "failed",
                "data_count": len(pfas_data.get('data', [])) if "error" not in pfas_data else 0,
                "summary": pfas_data.get('summary', {}) if "error" not in pfas_data else pfas_data
            },
            "calculated_summary": summary_stats
        })
        
    except Exception as e:
        app.logger.error(f"❌ Test summary failed for {sku}: {e}", exc_info=True)
        return jsonify({"error": str(e), "sku": sku})
    

# Enhanced test route to debug regulatory calculations
@app.route('/test-regulatory/<sku>')
def test_regulatory(sku):
    """Test route to check regulatory-based conformance calculation"""
    try:
        app.logger.info(f"🧪 Testing regulatory calculations for SKU: {sku}")
        
        # Check basic data counts
        bom_count = db.session.query(PFASBOM).filter_by(sku=sku).count()
        unique_materials = db.session.query(PFASBOM.material).filter_by(sku=sku).distinct().count()
        
        # Check chemical data
        chemicals_query = db.session.query(
            PFASMaterialChemicals.material_id,
            PFASMaterialChemicals.cas_number, 
            PFASMaterialChemicals.chemical_name,
            PFASMaterialChemicals.concentration_ppm
        ).join(PFASBOM, PFASMaterialChemicals.material_id == PFASBOM.material).filter(PFASBOM.sku == sku)
        
        chemicals = chemicals_query.all()
        
        # Check regulatory data
        regulatory_query = db.session.query(
            PFASRegulations.cas_number,
            PFASRegulations.australian_aics,
            PFASRegulations.australian_imap_tier_2,
            PFASRegulations.canadian_dsl,
            PFASRegulations.canada_pctsr_2012,
            PFASRegulations.eu_reach_pre_registered,
            PFASRegulations.eu_reach_registered_ppm,
            PFASRegulations.us_epa_tscainventory,
            PFASRegulations.us_epa_tsca12b
        ).join(PFASMaterialChemicals, PFASRegulations.cas_number == PFASMaterialChemicals.cas_number).join(
            PFASBOM, PFASMaterialChemicals.material_id == PFASBOM.material
        ).filter(PFASBOM.sku == sku)
        
        regulatory_data = regulatory_query.all()
        
        # Test the conformance calculation
        try:
            # Legacy/test route should call non-strict mode to reflect PFAS legacy behaviour
            conformance_stats = calculate_regulatory_conformance(sku, strict=False)
        except Exception as e:
            conformance_stats = {"error": str(e)}
        
        # Sample some data for inspection
        sample_chemicals = []
        for chem in chemicals[:5]:  # First 5 chemicals
            sample_chemicals.append({
                "material_id": chem.material_id,
                "cas_number": chem.cas_number,
                "chemical_name": chem.chemical_name,
                "concentration_ppm": chem.concentration_ppm
            })
        
        sample_regulatory = []
        for reg in regulatory_data[:5]:  # First 5 regulatory entries
            sample_regulatory.append({
                "cas_number": reg.cas_number,
                "australian_aics": reg.australian_aics,
                "australian_imap_tier_2": reg.australian_imap_tier_2,
                "canadian_dsl": reg.canadian_dsl,
                "canada_pctsr_2012": reg.canada_pctsr_2012,
                "eu_reach_pre_registered": reg.eu_reach_pre_registered,
                "eu_reach_registered_ppm": reg.eu_reach_registered_ppm,
                "us_epa_tscainventory": reg.us_epa_tscainventory,
                "us_epa_tsca12b": reg.us_epa_tsca12b
            })
        
        return jsonify({
            "sku": sku,
            "basic_counts": {
                "bom_records": bom_count,
                "unique_materials": unique_materials,
                "chemical_records": len(chemicals),
                "regulatory_records": len(regulatory_data)
            },
            "sample_data": {
                "chemicals": sample_chemicals,
                "regulatory": sample_regulatory
            },
            "conformance_calculation": conformance_stats,
            "summary_calculation": {
                "status": "attempting...",
                "result": "Check logs for detailed calculation steps"
            }
        })
        
    except Exception as e:
        app.logger.error(f"❌ Test regulatory failed for {sku}: {e}", exc_info=True)
        return jsonify({"error": str(e), "sku": sku})


# Also add a simplified debug route to see raw query results
@app.route('/debug-raw-data/<sku>')
def debug_raw_data(sku):
    """Debug route to see raw data from the main regulatory query"""
    try:
        # The exact query used in calculate_regulatory_conformance
        results = db.session.query(
            PFASBOM.material,
            PFASBOM.material_name,
            PFASMaterialChemicals.cas_number,
            PFASMaterialChemicals.chemical_name,
            PFASMaterialChemicals.concentration_ppm,
            PFASRegulations.australian_aics,
            PFASRegulations.australian_imap_tier_2,
            PFASRegulations.canadian_dsl,
            PFASRegulations.canada_pctsr_2012,
            PFASRegulations.eu_reach_pre_registered,
            PFASRegulations.eu_reach_registered_ppm,
            PFASRegulations.us_epa_tscainventory,
            PFASRegulations.us_epa_tsca12b
        ).outerjoin(
            PFASMaterialChemicals, PFASBOM.material == PFASMaterialChemicals.material_id
        ).outerjoin(
            PFASRegulations, PFASMaterialChemicals.cas_number == PFASRegulations.cas_number
        ).filter(PFASBOM.sku == sku).limit(10).all()  # Limit to first 10 for readability
        
        formatted_results = []
        for row in results:
            formatted_results.append({
                "material": row.material,
                "material_name": row.material_name,
                "cas_number": row.cas_number,
                "chemical_name": row.chemical_name,
                "concentration_ppm": row.concentration_ppm,
                "regulatory_thresholds": {
                    "australian_aics": row.australian_aics,
                    "australian_imap_tier_2": row.australian_imap_tier_2,
                    "canadian_dsl": row.canadian_dsl,
                    "canada_pctsr_2012": row.canada_pctsr_2012,
                    "eu_reach_pre_registered": row.eu_reach_pre_registered,
                    "eu_reach_registered_ppm": row.eu_reach_registered_ppm,
                    "us_epa_tscainventory": row.us_epa_tscainventory,
                    "us_epa_tsca12b": row.us_epa_tsca12b
                }
            })
        
        return jsonify({
            "sku": sku,
            "query_results_sample": formatted_results,
            "total_query_results": db.session.query(
                PFASBOM.material
            ).outerjoin(
                PFASMaterialChemicals, PFASBOM.material == PFASMaterialChemicals.material_id
            ).outerjoin(
                PFASRegulations, PFASMaterialChemicals.cas_number == PFASRegulations.cas_number
            ).filter(PFASBOM.sku == sku).count()
        })
        
    except Exception as e:
        return jsonify({"error": str(e), "sku": sku})
    
# ==================== FILTER PAGE API ENDPOINTS ====================
@app.route('/api/skus')
def get_skus():
    try:
        results = db.session.query(PFASBOM.sku, PFASBOM.product).distinct(PFASBOM.sku).all()
        data = [f"{r.sku} - {r.product}" for r in results]
        return jsonify({"skus": data})
    except Exception as e:
        app.logger.error(f"Error fetching SKUs: {e}")
        return jsonify({"skus": []}), 500
    
@app.route('/api/components')
def get_components():
    skus = request.args.get('skus', '')
    if not skus:
        return jsonify({"components": []})
    
    sku_list = [s.split(' - ')[0] for s in skus.split(',')]
    
    try:
        results = db.session.query(
            PFASBOM.component,
            PFASBOM.component_description
        ).filter(PFASBOM.sku.in_(sku_list)).distinct().all()
        
        components = [f"{r.component} - {r.component_description}" for r in results]
        return jsonify({"components": components})
    except Exception as e:
        app.logger.error(f"Error fetching components: {e}")
        return jsonify({"components": []}), 500

    
@app.route('/api/subcomponents')
def get_subcomponents():
    skus = request.args.get('skus', '')
    components = request.args.get('components', '')
    
    if not skus or not components:
        return jsonify({"subcomponents": []})
    
    sku_list = [s.split(' - ')[0] for s in skus.split(',')]
    comp_list = [c.split(' - ')[0] for c in components.split(',')]
    
    try:
        results = db.session.query(
            PFASBOM.subcomponent,
            PFASBOM.subcomponent_description
        ).filter(
            PFASBOM.sku.in_(sku_list),
            PFASBOM.component.in_(comp_list)
        ).distinct().all()
        
        subcomponents = [f"{r.subcomponent} - {r.subcomponent_description}" for r in results]
        return jsonify({"subcomponents": subcomponents})
    except Exception as e:
        app.logger.error(f"Error fetching subcomponents: {e}")
        return jsonify({"subcomponents": []}), 500

@app.route('/api/materials')
def get_materials():
    skus = request.args.get('skus', '')
    components = request.args.get('components', '')
    subcomponents = request.args.get('subcomponents', '')
    
    if not all([skus, components, subcomponents]):
        return jsonify({"materials": []})
    
    sku_list = [s.split(' - ')[0] for s in skus.split(',')]
    comp_list = [c.split(' - ')[0] for c in components.split(',')]
    sub_list = [s.split(' - ')[0] for s in subcomponents.split(',')]

    try:
        results = db.session.query(
            PFASBOM.material,
            PFASBOM.material_name
        ).filter(
            PFASBOM.sku.in_(sku_list),
            PFASBOM.component.in_(comp_list),
            PFASBOM.subcomponent.in_(sub_list)
        ).distinct().all()
        
        materials = [f"{r.material} - {r.material_name}" for r in results]
        return jsonify({"materials": materials})
    except Exception as e:
        app.logger.error(f"Error fetching materials: {e}")
        return jsonify({"materials": []}), 500

@app.route('/api/export-excel/<sku>')
def export_excel(sku):
    try:
        # Fetch assessment data by calling the internal function and extracting JSON
        resp = pfas_assessment(sku)
        if hasattr(resp, 'status_code') and resp.status_code != 200:
            app.logger.error(f"export_excel: pfas_assessment failed for {sku}: {resp.get_data(as_text=True)}")
            return jsonify({'error': 'Assessment failed'}), 400

        assessment_json = resp.get_json() if hasattr(resp, 'get_json') else resp
        data = assessment_json.get('data', []) if isinstance(assessment_json, dict) else []

        # Create a DataFrame
        df = pd.DataFrame(data)

        # Create an Excel file in memory
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='PFAS Assessment')

        output.seek(0)

        return send_file(
            output,
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            as_attachment=True,
            download_name=f'PFAS-Assessment-{sku}.xlsx'
        )
    except Exception as e:
        print(f"Error exporting to Excel: {str(e)}")
        return jsonify({'error': str(e)}), 500
    
@app.route('/api/filter-results')
def filter_results():
    skus = request.args.get('skus', '')
    components = request.args.get('components', '')
    subcomponents = request.args.get('subcomponents', '')
    materials = request.args.get('materials', '')
    region = request.args.get('region', '')
    valid_columns = [
        'australian_aics',
        'australian_imap_tier_2',
        'canadian_dsl',
        'canada_pctsr_2012',
        'eu_reach_pre_registered',
        'eu_reach_registered_ppm',
        'us_epa_tscainventory',
        'us_epa_tsca12b'
    ]
    if not skus or region not in valid_columns:
        return jsonify({"data": []})
    sku_list = [s.split(' - ')[0] for s in skus.split(',')]
    comp_list = [c.split(' - ')[0] for c in components.split(',')] if components else None
    sub_list = [s.split(' - ')[0] for s in subcomponents.split(',')] if subcomponents else None
    mat_list = [m.split(' - ')[0] for m in materials.split(',')] if materials else None

    try:
        # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
        # STEP 1: If materials are selected, check flags and trigger ingestion (EXISTING LOGIC)
        if mat_list:
            app.logger.info(f"🔄 Checking flags for {len(mat_list)} materials before generating filter report...")
            for material_id in mat_list:
                flag_entry = db.session.query(PFASBOM.flag).filter_by(material=material_id).first()
                if flag_entry and flag_entry.flag is False:
                    app.logger.info(f"🔄 Flag is False for material {material_id}. Calling FastAPI /ingest...")
                    fastapi_response = ingest_material_data(material_id)
                    if fastapi_response and fastapi_response.get("success", False):
                        should_set_flag_true = fastapi_response.get("flag", False)
                        if should_set_flag_true:
                            try:
                                db.session.query(PFASBOM).filter_by(material=material_id).update({"flag": True})
                                db.session.commit()
                                app.logger.info(f"✅ Flag updated to True for material_id: {material_id}")
                            except Exception as update_error:
                                db.session.rollback()
                                app.logger.error(f"⚠️ Failed to update flag for {material_id}: {update_error}")
                        else:
                            app.logger.info(f"ℹ️ Ingestion successful for {material_id}, but FastAPI flag=False. Database flag unchanged.")
                    else:
                        error_detail = "No response" if fastapi_response is None else fastapi_response.get("message", "Unknown error")
                        app.logger.warning(f"⚠️ Ingestion NOT successful for material_id {material_id}. Reason: {error_detail}")
        # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

        # STEP 2: Build and execute the main query (UNCHANGED)
        query = db.session.query(
            PFASBOM.sku,
            PFASBOM.product,
            PFASBOM.component,
            PFASBOM.component_description,
            PFASBOM.subcomponent,
            PFASBOM.subcomponent_description,
            PFASBOM.material,
            PFASBOM.material_name,
            PFASMaterialChemicals.chemical_name,
            PFASMaterialChemicals.cas_number,
            PFASMaterialChemicals.concentration_ppm,
            PFASMaterialChemicals.supplier_name,
            getattr(PFASRegulations, region).label("limit_value")
        ).join(
            PFASMaterialChemicals, PFASBOM.material == PFASMaterialChemicals.material_id
        ).outerjoin(
            PFASRegulations, PFASMaterialChemicals.cas_number == PFASRegulations.cas_number
        ).filter(PFASBOM.sku.in_(sku_list))

        if comp_list:
            query = query.filter(PFASBOM.component.in_(comp_list))
        if sub_list:
            query = query.filter(PFASBOM.subcomponent.in_(sub_list))
        if mat_list:
            query = query.filter(PFASBOM.material.in_(mat_list))

        results = query.all()

        # STEP 3: Process results — UPDATED to handle "Unknown" and mark as Non-Compliant
        data = []
        for r in results:
            # ✅ Handle concentration: if missing or invalid, set to None (will become "Unknown")
            try:
                conc = int(float(r.concentration_ppm)) if r.concentration_ppm is not None else None
            except (ValueError, TypeError):
                conc = None

            limit = float(r.limit_value) if r.limit_value is not None else None

            # 🚨 CRITICAL: If concentration is unknown, mark as NON-COMPLIANT
            if conc is None:
                status = "Non-Compliant"
                color = "danger"
            elif limit is None:
                status = "No Data"
                color = "warning"
            elif limit < conc:
                status = "Non-Compliant"
                color = "danger"
            else:
                status = "Compliant"
                color = "success"

            data.append({
                "sku": f"{r.sku} - {r.product}" if r.product else r.sku,
                "component": f"{r.component} - {r.component_description}" if r.component_description else r.component,
                "subcomponent": f"{r.subcomponent} - {r.subcomponent_description}" if r.subcomponent_description else r.subcomponent,
                "material": r.material or "Unknown",                          # ✅ Fixed
    "material_name": r.material_name or "Unknown", 
                "cas_number": r.cas_number or "Unknown",
                "chemical_name": r.chemical_name or "Unknown",
                "concentration": f"{int(conc)} ppm" if conc is not None else "Unknown",  # ✅ Show "Unknown"
                "supplier_name": r.supplier_name or "Unknown",
                "status": status,
                "status_color": color
            })

        return jsonify({"data": data})

    except Exception as e:
        app.logger.error(f"Filter results error: {e}")
        return jsonify({"data": []}), 500
    
@app.route('/api/export-filter-results', methods=['POST'])
def export_filter_results():
    try:
        app.logger.info("📥 Received export request")

        payload = request.get_json()
        if not payload or "filters" not in payload:
            return jsonify({"error": "Missing filters in request"}), 400

        filters = payload["filters"]
        app.logger.info(f"🔍 Filters received: {filters}")

        # Parse filters (strip " - description" if present)
        skus = [s.split(" - ")[0] for s in filters.get("skus", [])]
        components = [c.split(" - ")[0] for c in filters.get("components", [])]
        subcomponents = [sc.split(" - ")[0] for sc in filters.get("subcomponents", [])]
        materials = [m.split(" - ")[0] for m in filters.get("materials", [])]

        if not skus:
            return jsonify({"error": "No valid SKUs provided"}), 400

        # Regulation columns to include
        regulation_columns = [
            "australian_aics",
            "australian_imap_tier_2",
            "canadian_dsl",
            "canada_pctsr_2012",
            "eu_reach_pre_registered",
            "eu_reach_registered_ppm",
            "us_epa_tscainventory",
            "us_epa_tsca12b"
        ]

        # Build query
        query = db.session.query(
            PFASBOM.sku,
            PFASBOM.product,
            PFASBOM.component,
            PFASBOM.component_description,
            PFASBOM.subcomponent,
            PFASBOM.subcomponent_description,
            PFASBOM.material,
            PFASBOM.material_name,
            PFASMaterialChemicals.cas_number,
            PFASMaterialChemicals.chemical_name,
            PFASMaterialChemicals.concentration_ppm,
            PFASMaterialChemicals.supplier_name,
            PFASMaterialChemicals.reference_doc,
            *[getattr(PFASRegulations, col) for col in regulation_columns]
        ).join(
            PFASMaterialChemicals, PFASBOM.material == PFASMaterialChemicals.material_id
        ).outerjoin(
            PFASRegulations, PFASMaterialChemicals.cas_number == PFASRegulations.cas_number
        ).filter(PFASBOM.sku.in_(skus))

        if components:
            query = query.filter(PFASBOM.component.in_(components))
        if subcomponents:
            query = query.filter(PFASBOM.subcomponent.in_(subcomponents))
        if materials:
            query = query.filter(PFASBOM.material.in_(materials))

        results = query.all()
        app.logger.info(f"📊 Query returned {len(results)} rows")

        if not results:
            return jsonify({"error": "No matching records found"}), 400

        # Convert query results → flat rows
        rows = []
        for r in results:
            conc = int(float(r.concentration_ppm)) if r.concentration_ppm else 0.0

            row = {
                "SKU": r.sku or "",
                "Product": r.product or "",
                "Component": r.component or "",
                "Component Description": r.component_description or "",
                "Sub-Component": r.subcomponent or "",
                "Sub-Component Description": r.subcomponent_description or "",
                "Material ID": r.material or "",
                "Material Name": r.material_name or "",
                "CAS Number": r.cas_number or "Unknown",
                "Chemical Name": r.chemical_name or "Unknown",
                "Chemical Concentration": f"{int(conc)} ppm",
                "Supplier Name": r.supplier_name or "Unknown",
                "Reference": r.reference_doc or "",
            }

            # Add regulation thresholds + status
            for col in regulation_columns:
                threshold = getattr(r, col, None)
                if threshold is None:
                    row[col] = "No Data"
                else:
                    threshold_val = float(threshold)
                    row[col] = f"{threshold_val} ppm"
                    row[f"{col}_status"] = "exceeded" if conc > threshold_val else "within"

            rows.append(row)

        # Convert to DataFrame (ignore *_status for sheet layout)
        df = pd.DataFrame([{k: v for k, v in row.items() if not k.endswith("_status")} for row in rows])

        # Write Excel
        output = BytesIO()
        with pd.ExcelWriter(output, engine="openpyxl") as writer:
            df.to_excel(writer, sheet_name="Results", index=False)
            ws = writer.sheets["Results"]

            # Coloring
            red_fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")
            green_fill = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")

            # Apply fills row by row
            for i, row in enumerate(rows, start=2):  # Excel rows start at 2 (after header)
                for col_idx, col in enumerate(regulation_columns, start=1):
                    cell = ws.cell(row=i, column=df.columns.get_loc(col) + 1)
                    status = row.get(f"{col}_status")
                    if status == "exceeded":
                        cell.fill = red_fill
                    elif status == "within":
                        cell.fill = green_fill

            # Auto column width
            for col_cells in ws.columns:
                max_length = max(len(str(cell.value)) if cell.value else 0 for cell in col_cells)
                ws.column_dimensions[col_cells[0].column_letter].width = min(max_length + 2, 50)

        output.seek(0)
        filename = f"PFAS_Filter_Report_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}.xlsx"

        app.logger.info(f"✅ Export successful: {len(rows)} rows exported")
        return send_file(
            output,
            as_attachment=True,
            download_name=filename,
            mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

    except Exception as e:
        app.logger.error(f"❌ Export failed: {e}", exc_info=True)
        return jsonify({"error": "Export failed. Please try again."}), 500

# Add these new routes to your app.py

# Add these new routes to your app.py (or replace the existing ones if duplicated)
@app.route('/api/assessment-regions/<sku>')
def get_assessment_regions(sku):
    """Get available regions for the assessment page"""
    try:
        # Define region mappings - corrected to match database columns and frontend keys
        region_mappings = {
            'australia_aics': 'Australia (AICS)',
            'australia_imap_tier_2': 'Australia (IMAP Tier 2)', # Corrected key
            'canadian_dsl': 'Canada (DSL)',
            'canada_pctsr_2012': 'Canada (PCTSR 2012)', # Corrected key
            'eu_reach_pre_registered': 'EU (REACH Pre-registered)', # Corrected key
            'eu_reach_registered_ppm': 'EU (REACH Registered)', # Corrected key
            'us_epa_tscainventory': 'USA (EPA TSCA Inventory)', # Corrected key
            'us_epa_tsca12b': 'USA (EPA TSCA 12B)' # Corrected key
        }

        # Column to frontend key mapping (database column name -> region_mappings key)
        column_to_region_key = {
            'australian_aics': 'australia_aics',
            'australian_imap_tier_2': 'australia_imap_tier_2', # Corrected
            'canadian_dsl': 'canadian_dsl',
            'canada_pctsr_2012': 'canada_pctsr_2012', # Corrected
            'eu_reach_pre_registered': 'eu_reach_pre_registered', # Corrected
            'eu_reach_registered_ppm': 'eu_reach_registered_ppm', # Corrected
            'us_epa_tscainventory': 'us_epa_tscainventory', # Corrected
            'us_epa_tsca12b': 'us_epa_tsca12b' # Corrected
        }

        # Check which regulatory columns have data for this SKU
        available_regions = []

        # Query to check which regulatory columns have non-null values for materials in this SKU
        # Use distinct to avoid checking the same regulation row multiple times if a chemical is used in multiple BOM entries
        # Select distinct cas_numbers relevant to this SKU first
        relevant_cas_numbers = db.session.query(PFASMaterialChemicals.cas_number).\
            join(PFASBOM, PFASMaterialChemicals.material_id == PFASBOM.material).\
            filter(PFASBOM.sku == sku).\
            distinct()

        # Instead of pulling all regulation rows and scanning them (which can be
        # fragile depending on how SQLAlchemy returns tuples), explicitly check
        # for each regulation column whether any PFASRegulations entry exists
        # (for CAS numbers present in this SKU) with a non-null value. This
        # is more robust and will correctly surface regions that have data.
        found_regions = set()
        for db_col, region_key in column_to_region_key.items():
            try:
                col_attr = getattr(PFASRegulations, db_col)
            except AttributeError:
                # Skip if mapping is wrong
                continue

            exists_q = db.session.query(PFASRegulations).filter(
                PFASRegulations.cas_number.in_(relevant_cas_numbers.subquery()),
                col_attr.isnot(None)
            ).limit(1).count()

            if exists_q and region_key in region_mappings:
                found_regions.add(region_key)

        # Convert to the format expected by frontend
        for region_key in sorted(found_regions):
            available_regions.append({
                'value': region_key,
                'label': region_mappings.get(region_key, region_key)
            })

        app.logger.info(f"Available regions for SKU {sku}: {available_regions}")
        return jsonify({
            'regions': available_regions,
            'default': 'all'
        })

    except Exception as e:
        app.logger.error(f"Error getting regions for {sku}: {e}", exc_info=True)
        return jsonify({'regions': [], 'default': 'all'}), 500


@app.route('/api/assessment-filter/<sku>')
def filter_assessment_by_region(sku):
    """Filter assessment data by region"""
    try:
        region_filter = request.args.get('region', 'all')

        app.logger.info(f"Filtering assessment for SKU {sku} by region: {region_filter}")

        # Define column mappings for regions - corrected keys to match database columns
        region_to_columns = {
            'australia_aics': 'australian_aics', # Map frontend key to actual DB column
            'australia_imap_tier_2': 'australian_imap_tier_2', # Corrected
            'canadian_dsl': 'canadian_dsl', # Corrected
            'canada_pctsr_2012': 'canada_pctsr_2012', # Corrected
            'eu_reach_pre_registered': 'eu_reach_pre_registered', # Corrected
            'eu_reach_registered_ppm': 'eu_reach_registered_ppm', # Corrected
            'us_epa_tscainventory': 'us_epa_tscainventory', # Corrected
            'us_epa_tsca12b': 'us_epa_tsca12b' # Corrected
        }

        # Define regulations for display and processing - ensure keys match region_to_columns keys
        regulations_definitions = [
            {'name': 'Australian AICS', 'col': 'australian_aics', 'region_key': 'australia_aics'},
            {'name': 'Australian IMAP Tier 2', 'col': 'australian_imap_tier_2', 'region_key': 'australia_imap_tier_2'}, # Corrected
            {'name': 'Canadian DSL', 'col': 'canadian_dsl', 'region_key': 'canadian_dsl'}, # Corrected
            {'name': 'Canada PCTSR 2012', 'col': 'canada_pctsr_2012', 'region_key': 'canada_pctsr_2012'}, # Corrected
            {'name': 'EU REACH Pre Registered', 'col': 'eu_reach_pre_registered', 'region_key': 'eu_reach_pre_registered'}, # Corrected
            {'name': 'EU REACH Registered', 'col': 'eu_reach_registered_ppm', 'region_key': 'eu_reach_registered_ppm'}, # Corrected
            {'name': 'US EPA TSCA Inventory', 'col': 'us_epa_tscainventory', 'region_key': 'us_epa_tscainventory'}, # Corrected
            {'name': 'US EPA TSCA 12B', 'col': 'us_epa_tsca12b', 'region_key': 'us_epa_tsca12b'} # Corrected
        ]

        # Filter regulations based on selected region
        filtered_regulations = regulations_definitions
        if region_filter != 'all' and region_filter in region_to_columns:
             # Filter regulations to only include the one selected
            filtered_regulations = [reg for reg in regulations_definitions if reg['region_key'] == region_filter]
            app.logger.debug(f"Filtered regulations for {region_filter}: {[r['name'] for r in filtered_regulations]}")

        # Base query - Get all BOM entries for the SKU, joined with chemicals and regulations
        query = db.session.query(
            PFASBOM.component,
            PFASBOM.subcomponent,
            PFASBOM.material,
            PFASBOM.material_name,
            PFASMaterialChemicals.cas_number,
            PFASMaterialChemicals.chemical_name,
            PFASMaterialChemicals.concentration_ppm,
            PFASMaterialChemicals.supplier_name,
            PFASMaterialChemicals.reference_doc,
            PFASRegulations.australian_aics,
            PFASRegulations.australian_imap_tier_2,
            PFASRegulations.canadian_dsl,
            PFASRegulations.canada_pctsr_2012,
            PFASRegulations.eu_reach_pre_registered,
            PFASRegulations.eu_reach_registered_ppm,
            PFASRegulations.us_epa_tscainventory,
            PFASRegulations.us_epa_tsca12b
        ).outerjoin(
            PFASMaterialChemicals, PFASBOM.material == PFASMaterialChemicals.material_id
        ).outerjoin(
            PFASRegulations, PFASMaterialChemicals.cas_number == PFASRegulations.cas_number
        ).filter(PFASBOM.sku == sku)

        # Apply region filter if not 'all'
        # This means we only want to show rows where the selected regulation column has data
        if region_filter != 'all' and region_filter in region_to_columns:
            db_column_name = region_to_columns[region_filter]
            db_column = getattr(PFASRegulations, db_column_name)
            query = query.filter(db_column.isnot(None))
            app.logger.debug(f"Applied filter for column: {db_column_name}")

        results = query.all()
        app.logger.debug(f"Query returned {len(results)} rows after filtering.")

        data = []
        non_conforming_count = 0
        in_conformance_count = 0
        no_data_count = 0 # Includes entries with no chemical data OR entries where the specific regulation has no data

        for row in results:
            # Process each row
            component = row.component or "-"
            subcomponent = row.subcomponent or "-"
            material = row.material or "-"
            material_name = row.material_name or "-"
            cas = row.cas_number or "Unknown"
            chem_name = row.chemical_name or "Unknown"

            try:
                conc = int(float(row.concentration_ppm)) if row.concentration_ppm is not None else None
            except (ValueError, TypeError):
                conc = None

            supplier = row.supplier_name or "Unknown"
            has_chemical_data = row.cas_number is not None or row.chemical_name is not None

            limits = []
            is_non_conforming = False
            regulation_has_data = False # Flag to check if any of the *applied* regulations have data for this row

            # Loop through the *filtered* regulations list
            for reg_def in filtered_regulations:
                db_col_name = reg_def['col']
                value = getattr(row, db_col_name, None)
                threshold = float(value) if value is not None else None

                # Mark that at least one regulation column we are looking at has data
                if value is not None:
                    regulation_has_data = True

                if conc is None:
                    # 🚨 CRITICAL CHANGE: If concentration is unknown, mark as NON-CONFORMING for ALL applicable regulations
                    status = 'exceeded'
                    color = 'danger'
                    limit_display = f"{threshold} ppm" if threshold is not None else "No Threshold"
                    is_non_conforming = True
                else:
                    # Only calculate compliance if we have a valid concentration
                    if threshold is None:
                        status = 'unknown'
                        color = 'warning'
                        limit_display = "No Data"
                        # Don't set is_non_conforming here, as "No Data" means we don't know, not that it fails
                    elif threshold < conc:
                        status = 'exceeded'
                        color = 'danger'
                        is_non_conforming = True
                        limit_display = f"{threshold} ppm"
                    else:
                        status = 'within'
                        color = 'success'
                        limit_display = f"{threshold} ppm"

                limits.append({
                    'name': reg_def['name'],
                    'limit': limit_display,
                    'status': status,
                    'color': color
                })

            # Categorize entry based on results for the *filtered* regulations
            if not has_chemical_data:
                # No chemical data at all for this material entry
                no_data_count += 1
                entry_status = "No Chemical Data"
                status_color = "warning"
            elif not regulation_has_data and region_filter != 'all':
                 # Chemical data exists, but no data for the *specific* regulation we are filtering by
                 # Only count this if we are actually filtering by a region
                no_data_count += 1
                entry_status = "No Data for Region"
                status_color = "warning"
            elif is_non_conforming:
                # Failed one or more of the applicable regulations
                non_conforming_count += 1
                entry_status = "Non-Conforming"
                status_color = "danger"
            else:
                # Passed all applicable regulations (or no specific regulations were applied if 'all')
                in_conformance_count += 1
                entry_status = "Conforming"
                status_color = "success"

            data.append({
                "component": component,
                "subcomponent": subcomponent,
                "material": material,
                "material_name": material_name,
                "supplier_name": supplier,
                "chemical_name": chem_name,
                "cas_number": cas,
                "concentration": f"{int(conc)} ppm" if conc is not None else "Unknown",
                "reference_doc": row.reference_doc or "—",
                "limits": limits,
                "status": entry_status,
                "status_color": status_color
            })

        # Calculate summary statistics
        total_materials = db.session.query(PFASBOM.material).filter_by(sku=sku).distinct().count()
        materials_with_chemicals = db.session.query(PFASBOM.material).filter_by(sku=sku).join(
            PFASMaterialChemicals, PFASBOM.material == PFASMaterialChemicals.material_id
        ).distinct().count()

        summary = {
            'total': len(data),
            'non_conforming': non_conforming_count,
            'in_conformance': in_conformance_count,
            'no_chemical_data': no_data_count, # This now includes "No Data for Region"
            'files_total': total_materials,
            'files_downloaded': materials_with_chemicals,
            'files_not_found': max(0, total_materials - materials_with_chemicals),
            # These last two are derived from conformance counts, adjust logic if needed
            'alt_found': max(0, in_conformance_count // 3),
            'alt_not_found': max(0, non_conforming_count // 4)
        }

        return jsonify({
            'success': True,
            'region': region_filter,
            'data': data,
            'summary': summary,
            'regulations_applied': [reg['name'] for reg in filtered_regulations]
        })

    except Exception as e:
        app.logger.error(f"Error filtering assessment by region for SKU {sku}: {e}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/assessment/<sku>')
def assessment_page(sku):
    """Render the standalone assessment page with optional region filtering."""
    try:
        # Get region parameter for server-side fallback
        region_param = request.args.get('region', 'all')

        # Existing BOM processing logic (unchanged)
        bom_entries = db.session.query(PFASBOM.material, PFASBOM.flag).filter_by(sku=sku).distinct().all()
        if not bom_entries:
            flash("No BOM data found for this product.", "warning")
            return redirect(url_for('index'))

        material_ids = list(set([entry.material for entry in bom_entries]))
        app.logger.info(f"Processing {len(material_ids)} unique materials for SKU {sku}")

        # Handle material ingestion (existing logic)
        processed_materials = set()
        for material_id in material_ids:
            if material_id in processed_materials:
                continue
            processed_materials.add(material_id)

            flag_entry = db.session.query(PFASBOM.flag).filter_by(material=material_id).first()
            if flag_entry and flag_entry.flag is False:
                app.logger.info(f"Flag is False for material {material_id}. Calling FastAPI /ingest...")
                fastapi_response = ingest_material_data(material_id)
                if fastapi_response and fastapi_response.get("success", False):
                    should_set_flag_true = fastapi_response.get("flag", False)
                    if should_set_flag_true:
                        try:
                            db.session.query(PFASBOM).filter_by(material=material_id).update({"flag": True})
                            db.session.commit()
                            app.logger.info(f"Flag updated to True for material_id: {material_id}")
                        except Exception as update_error:
                            db.session.rollback()
                            app.logger.error(f"Failed to update flag for {material_id}: {update_error}", exc_info=True)

        # Get assessment data (potentially filtered by region)
        # The logic here is slightly different from the API call. It calls the API route internally.
        # This might be okay for server-side rendering, but ensure it's efficient.
        # Alternatively, refactor pfas_assessment or create a shared data processing function.

        # Decide whether to use filtered data or full data based on region_param
        def _extract_json_from_response(resp):
            """Helper to normalize the different return shapes we may get from internal calls.
            It handles:
              - Flask Response objects (has get_json and status_code)
              - Tuples like (Response, status_code)
              - Plain dicts
            Returns (data_dict, status_code)
            """
            # If it's already a dict
            if isinstance(resp, dict):
                return resp, 200

            # If function returned a tuple (body, status)
            if isinstance(resp, tuple) and len(resp) >= 1:
                body = resp[0]
                status = resp[1] if len(resp) > 1 and isinstance(resp[1], int) else None
                if hasattr(body, 'get_json'):
                    try:
                        return body.get_json(), status or getattr(body, 'status_code', 200)
                    except Exception:
                        return {"error": "Invalid JSON in internal response"}, status or 500
                elif isinstance(body, dict):
                    return body, status or 200
                else:
                    return {"error": "Unexpected response type from internal call"}, status or 500

            # If it's a Flask Response-like object
            if hasattr(resp, 'get_json') and hasattr(resp, 'status_code'):
                try:
                    return resp.get_json(), resp.status_code
                except Exception:
                    return {"error": "Invalid JSON in internal response"}, resp.status_code

            # Fallback
            return {"error": "Unknown response type from internal call"}, 500

        if region_param != 'all':
            # Call the filter API internally and normalize result
            filter_resp = filter_assessment_by_region(sku)
            filter_data, filter_status = _extract_json_from_response(filter_resp)
            if filter_status == 200 and filter_data.get('success', False):
                data = {
                    'product': f"{sku}_Filtered",
                    'summary': filter_data.get('summary', {}),
                    'data': filter_data.get('data', [])
                }
            else:
                app.logger.warning(f"API filter failed for {sku} (status={filter_status}) - response: {filter_data}")
                # Fallback to full assessment
                resp = pfas_assessment(sku)
                data, status = _extract_json_from_response(resp)
        else:
            resp = pfas_assessment(sku)
            data, status = _extract_json_from_response(resp)
            if status != 200:
                app.logger.error(f"PFAS assessment failed for {sku}: status={status}, data={data}")

        if "error" in data:
            flash("Assessment data not found for this product.", "warning")
            return redirect(url_for('index'))

        # Calculate summary statistics using the potentially filtered data (legacy PFAS behaviour)
        # Note: This might recalculate summary based on the full dataset again.
        # Consider passing the summary from the filter API if it's the same logic.
        summary_stats = calculate_dynamic_summary(sku, data, strict=False)
        product_name = data.get("product", f"{sku}_Unknown")

        # Pass selected_region to template for initial state if needed by JS
        return render_template('assessment.html',
                             sku=sku,
                             product_name=product_name,
                             initial_data=data, # This contains the data to display initially
                             summary_stats=summary_stats, # This contains the summary to display initially
                             selected_region=region_param # Pass the region parameter for JS initialization
                             )

    except Exception as e:
        app.logger.error(f"Unhandled error in assessment_page for {sku}: {e}", exc_info=True)
        flash("Failed to load assessment. Please try again later.", "danger")
        return redirect(url_for('index'))


@app.route('/static/templates/ppwr_bom_template.xlsx')
def download_ppwr_template():
    """Serve the PPWR BOM template from Helper_Data as a downloadable file.
    This avoids duplicating the file into the frontend static folder.
    """
    try:
        helper_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'Helper_Data'))
        filename = 'BOM-1_VET SYRINGE (3).xlsx'
        file_path = os.path.join(helper_dir, filename)
        if not os.path.exists(file_path):
            app.logger.error(f"PPWR template not found at {file_path}")
            return jsonify({'error': 'Template not found'}), 404

        return send_file(
            file_path,
            as_attachment=True,
            download_name='ppwr_bom_template.xlsx',
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )
    except Exception as e:
        app.logger.error(f"Error serving PPWR template: {e}", exc_info=True)
        return jsonify({'error': 'Failed to serve template'}), 500


@app.route('/ppwr-assessment/<sku>')
def ppwr_assessment_page(sku):
    """Render the PPWR assessment page which applies the stricter (PPWR) conformance rules.

    This re-uses the PFAS assessment data as a base but applies strict rules where unknown
    concentrations count as non-conforming. The dynamic summary is calculated in strict mode.
    """
    try:
        # Get region parameter for server-side fallback
        region_param = request.args.get('region', 'all')

        # Re-use internal pfas_assessment to get the base data structure
        resp = pfas_assessment(sku)

        # Normalize response shapes (dict, Flask Response, tuple)
        def _extract(resp_obj):
            if isinstance(resp_obj, dict):
                return resp_obj, 200
            if isinstance(resp_obj, tuple) and len(resp_obj) >= 1:
                body = resp_obj[0]
                status = resp_obj[1] if len(resp_obj) > 1 and isinstance(resp_obj[1], int) else 200
                if hasattr(body, 'get_json'):
                    try:
                        return body.get_json(), status
                    except Exception:
                        return {"error": "Invalid JSON in internal response"}, 500
                elif isinstance(body, dict):
                    return body, status
                else:
                    return {"error": "Unexpected internal response type"}, 500
            if hasattr(resp_obj, 'get_json') and hasattr(resp_obj, 'status_code'):
                try:
                    return resp_obj.get_json(), resp_obj.status_code
                except Exception:
                    return {"error": "Invalid JSON in internal response"}, 500
            return {"error": "Unknown response type"}, 500

        data, status = _extract(resp)
        if status != 200 or "error" in data:
            flash("PPWR assessment data not found for this product.", "warning")
            return redirect(url_for('index'))

        # Post-process rows to enforce strict PPWR rules: unknown concentration => NON-CONFORMING
        for entry in data.get('data', []):
            conc_raw = entry.get('concentration', '')
            # treat empty or 'Unknown' as missing
            if not conc_raw or str(conc_raw).strip().lower() == 'unknown':
                entry['status'] = 'Non-Conforming'
                entry['status_color'] = 'danger'

        # Calculate strict summary
        summary_stats = calculate_dynamic_summary(sku, data, strict=True)

        product_name = data.get('product', f"{sku}_PPWR")

        return render_template('assessment.html',
                               sku=sku,
                               product_name=product_name,
                               initial_data=data,
                               summary_stats=summary_stats,
                               selected_region=region_param)

    except Exception as e:
        app.logger.error(f"Unhandled error in ppwr_assessment_page for {sku}: {e}", exc_info=True)
        flash("Failed to load PPWR assessment. Please try again later.", "danger")
        return redirect(url_for('index'))

@app.errorhandler(404)
def not_found(error):
    return jsonify({'error': 'Endpoint not found'}), 404

@app.errorhandler(500)
def internal_error(error):
    return jsonify({'error': 'Internal server error'}), 500

@app.route('/logs')
def view_logs():
    """Render a simple log viewer for the frontend app logs.

    Reads the last N lines from frontend/logs/app.log and displays them with basic formatting.
    """
    try:
        logs_dir = os.path.join(os.path.dirname(__file__), 'logs')
        log_path = os.path.join(logs_dir, 'app.log')
        lines = []
        if os.path.exists(log_path):
            with open(log_path, 'r', encoding='utf-8', errors='replace') as f:
                lines = f.readlines()
        # Show last 400 lines to keep page light
        tail = lines[-400:] if lines else []
        return render_template('logs.html', log_lines=tail)
    except Exception as e:
        app.logger.error(f"Failed to render logs: {e}", exc_info=True)
        flash('Unable to load logs', 'danger')
        return render_template('logs.html', log_lines=[])

if __name__ == '__main__':
    # Robust startup: retry DB initialization a few times to avoid noisy errors
    # when the database container isn't ready yet.
    max_attempts = 10
    delay_seconds = 2

    with app.app_context():
        for attempt in range(1, max_attempts + 1):
            try:
                db.create_all()
                # Ensure the pfas_bom table has the uploaded_at column (safe idempotent alter)
                db.session.execute(text("ALTER TABLE pfas_bom ADD COLUMN IF NOT EXISTS uploaded_at TIMESTAMP"))
                db.session.commit()
                app.logger.info("Database tables created and uploaded_at ensured (attempt %d)", attempt)
                break
            except Exception as e:
                db.session.rollback()
                app.logger.warning(f"Database init attempt {attempt} failed: {e}")
                if attempt == max_attempts:
                    app.logger.error("Giving up on DB init after %d attempts; app will still start.", max_attempts)
                else:
                    import time
                    time.sleep(delay_seconds)

    # Allow overriding port via PORT env var for local runs
    try:
        run_port = int(os.environ.get('PORT', '5000'))
    except Exception:
        run_port = 5000
    # Bind host: in Docker expose on 0.0.0.0, otherwise loopback
    try:
        running_in_docker = os.path.exists('/.dockerenv') or os.environ.get('DOCKER') == '1'
    except Exception:
        running_in_docker = False
    bind_host = '0.0.0.0' if running_in_docker else '127.0.0.1'
    app.run(debug=True, host=bind_host, port=run_port, use_reloader=False)